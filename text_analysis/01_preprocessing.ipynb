{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3e7ea21-6437-48e8-a9e4-3bdc05f709c9",
   "metadata": {},
   "source": [
    "# Python Text Analysis: Fundamentals, Part 1\n",
    "\n",
    "In this workshop series, we'll establish building blocks for performing text analysis in Python. These techniques lie in the domain of *natural language processing*, where we apply computational techniques to language written by humans in order to explain some of the underlying structure.\n",
    "\n",
    "So, the million dollar question: How exactly do we go about performing computational methods on words?\n",
    "\n",
    "This is ultimately a question of *representations*. Text naturally is represented as words, which are understandable to humans because we have a grammatical and syntactical structure we use to extract meaning from those words. However, most machine learning and data science techniques utilize numerical methods to extract patterns from large datasets. So, we need to find a way to convert the language into a numerical representation. We'll start with this goal in mind, and demonstrate how involved this process can be.\n",
    "\n",
    "We'll start this process by first importing text into Python. Then, we'll cover a variety of preprocessing steps you might want to use before proceeding with computational methods. In the next sequence of this workshop, we'll work with the bag-of-words, or the first numerical representation of text that we'll encounter in this workshop series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8914f4-9783-4661-9cc9-32daca53e1fd",
   "metadata": {},
   "source": [
    "## Importing Files Containing Text\n",
    "\n",
    "Text data we want to analyze will be stored in external files that need to be imported. These files will generally be text files (`.txt`) or comma separated value files (`.csv`).\n",
    "\n",
    "All the data used in this notebook are stored in a `data` folder that we need to access. We need to adjust our filepaths accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "366544be-4b56-4ed2-ba8e-aa4e41d5f95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = '../data/sowing_and_reaping.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b462052f-766c-434c-a262-efa7326d61c0",
   "metadata": {},
   "source": [
    "### Text Files\n",
    "\n",
    "We'll first start by importing \"Sowing and Reaping\" by Frances Harper, which is stored in a text file. Python has built-in functionality for importing text files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3dc4536-5e6c-4052-a44d-c36760714925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and read the text\n",
    "with open(text_path, 'r') as file:\n",
    "    raw_text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b8e3b1-f92e-4525-a5ba-f396fb61271a",
   "metadata": {},
   "source": [
    "We've stored the text file in an object called `raw_text`. We'll remove the front and end matter for better preprocessing later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ff4469c-6a66-44bb-bd82-f20455497141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the front and end matter\n",
    "sowing_and_reaping = raw_text[1114:684814]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414ef3f4-1c0c-4c79-918f-72175a1292d3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Challenge 1: Working with Strings\n",
    "\n",
    "* What type of object is `sowing_and_reaping`?\n",
    "* How many characters are in `sowing_and_reaping`?\n",
    "* How can we get the first 1000 characters of `sowing_and_reaping`?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e693ebf0-6d66-4650-b825-e27f6860a8b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# What type of object is sowing_and_reaping?\n",
    "type(sowing_and_reaping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e93270ce-e25b-41e9-a1f6-7432e6d0ed20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167642"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# How many characters are in sowing_and_reaping?\n",
    "len(sowing_and_reaping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3788bdcd-6671-4f46-85f3-e28b5dd0847b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I hear that John Andrews has given up his saloon; and a foolish thing\\nit was. He was doing a splendid business. What could have induced him?\"\\n\\n\"They say that his wife was bitterly opposed to the business. I don\\'t\\nknow, but I think it quite likely. She has never seemed happy since John\\nhas kept saloon.\"\\n\\n\"Well, I would never let any woman lead me by the nose. I would let her\\nknow that as the living comes by me, the way of getting it is my affair,\\nnot hers, as long as she is well provided for.\"\\n\\n\"All men are not alike, and I confess that I value the peace and\\nhappiness of my home more than anything else; and I would not like to\\nengage in any business which I knew was a source of constant pain to my\\nwife.\"\\n\\n\"But, what right has a woman to complain, if she has every thing she\\nwants. I would let her know pretty soon who holds the reins, if I had\\nsuch an unreasonable creature to deal with. I think as much of my wife\\nas any man, but I want her to know her place, and I know mine.\"\\n\\n\"What do yo'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# How can we get the first 1000 characters of sowing_and_reaping?\n",
    "sowing_and_reaping[:1000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97e1de5-7cc6-4861-9ca9-0b58996d25f2",
   "metadata": {},
   "source": [
    "### Comma Separated Value (CSV) Files\n",
    "\n",
    "Often, we may have data stored in \"dataframes\" or \"tables\", which consists of many samples (rows), each containing several features (columns). Among the features is likely a text column which contains the text of interest. These dataframes are often found as Comma Separated Value (CSV) files (and somewhat less frequently as tab separated value (TSV) files). In either case, there is some \"delimiter\" (i.e., a comma or tab) which helps separate entries from each other.\n",
    "\n",
    "The `pandas` package is the best package for dealing with dataframes in Python, and this package comes with its own function for reading CSV files. For example, let's read in a file containing many Tweets about airlines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c75449af-a3ad-48df-a64f-1325d2ef5c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "# Use pandas to import Tweets\n",
    "csv_path = '../data/airline_tweets.csv'\n",
    "tweets = pd.read_csv(csv_path, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb6e3107-9f0a-414b-8267-1d5fbb09e246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d201a3de-aaf6-4c21-97c8-5ac38517e0b5",
   "metadata": {},
   "source": [
    "Let's take a look at some of the Tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c32d507-ff14-4a2b-b7ae-613f3f76c69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@VirginAmerica What @dhepburn said.\n",
      "@VirginAmerica plus you've added commercials to the experience... tacky.\n",
      "@VirginAmerica I didn't today... Must mean I need to take another trip!\n"
     ]
    }
   ],
   "source": [
    "print(tweets['text'].iloc[0])\n",
    "print(tweets['text'].iloc[1])\n",
    "print(tweets['text'].iloc[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3972dfc7-ddb4-486e-ac48-d5a7593f7b91",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Challenge 2: Reading in Many Files\n",
    "\n",
    "The `data` folder contains another folder called `amazon`, which contains many `csv` files of Amazon reviews. Use a `for` loop to read in each dataframe. Do the following:\n",
    "\n",
    "* We've provided a path to the `amazon` folder, and a list of all the file names within the folder using the `os.listdir()` function.\n",
    "* Iterate over all these files, and import them using `pd.read_csv()`. You will need to use `os.path.join()` to create the correct path. Additionally, you need to provide `pandas` with the column names since they are not included in the reviews. We have create the `column_names` variable for you.\n",
    "* Extract the text column from each dataframe, and add then to the `reviews` list. \n",
    "* How many totals reviews do you obtain?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d9fa08c-7e28-4c3f-ab0c-1d1ab5ad4b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The os package has useful tools for file manipulation\n",
    "import os\n",
    "# Amazon review folder\n",
    "amazon_path = '../data/amazon'\n",
    "# List all the files in the amazon folder\n",
    "files = os.listdir(amazon_path)\n",
    "# Column names for each file\n",
    "column_names = ['id',\n",
    "                'product_id',\n",
    "                'user_id',\n",
    "                'profile_name',\n",
    "                'helpfulness_num',\n",
    "                'helpfulness_denom',\n",
    "                'score',\n",
    "                'time',\n",
    "                'summary',\n",
    "                'text']\n",
    "# Add each review text to this list\n",
    "reviews = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb680f3e-b82a-411c-aff0-79d2a6dfbc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    # Check that the file is actually a CSV file\n",
    "    if os.path.splitext(file)[1] == '.csv':\n",
    "        # YOUR CODE HERE\n",
    "        full_path = os.path.join(amazon_path, file)\n",
    "        reviews_df = pd.read_csv(full_path, sep=',', names=column_names)\n",
    "        text = list(reviews_df['text'])\n",
    "        reviews.extend(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce98479-c234-41bc-a2da-c90ab822b421",
   "metadata": {},
   "source": [
    "There are other file types which you may come across: `json`, `xml`, `html`, etc. There are packages you can use to import each other these. The main challenge, in most cases, is dealing with multiple files, and extracting the actual text you want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e88f2b-bdaa-4c28-ad13-4e790ccb6827",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Our goal is to convert a text representation to a numerical representation. However, language can be messy. There's a variety of preprocessing steps that we'd like to do before we get to the numerical representation.\n",
    "\n",
    "We will largely be using a package called Natural Language Toolkit, or `nltk`, to perform these operations. In some cases, we'll use basic Python.\n",
    "\n",
    "There are a host of natural language processing packages one can use. For example, one newer package is `spaCy`, which is extremely powerful. Our goal here is to not make you an expert in a variety of NLP packages, but to expose you to principles that are shared by all of them. In this way, you'll be better prepared to open up any new NLP package you might have to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fd3c0c-844b-4297-9edb-32bb441af9ec",
   "metadata": {},
   "source": [
    "### Installing `nltk`\n",
    "\n",
    "If this is your first time using `nltk`, we'll go through a couple steps to get set up. First, install `nltk` if you have not already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23185480-b884-4c26-95f0-e9e647280b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: click in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from nltk) (8.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Run if you do not have nltk installed\n",
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c7f258-c3b5-45a0-9de3-f0a9382bce8b",
   "metadata": {},
   "source": [
    "Next, we need to install a couple packages within `nltk`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff00fb9e-708b-43c8-9ec7-767228f34980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/blueraspberry/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/blueraspberry/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3915028e-7ab1-4851-80f1-185da224cc52",
   "metadata": {},
   "source": [
    "### Text Cleaning\n",
    "\n",
    "\"Text cleaning\" is a catch-all term for the process of performing relatively simple tasks in order to normalize our code. Text cleaning can mean a variety of different things depending on your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba11eee-c926-4591-bb32-d268646309ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### A Brief Introduction to Regular Expressions\n",
    "\n",
    "Before we dive into the specific text cleaning processes, let's briefly introduce regular expressions. We do this here since many text cleaning steps may require regular expressions, and many NLP libraries heavily use them under the hood.\n",
    "\n",
    "Regular expressions (regexes) are a powerful way of searching for specific string patterns in large corpora. They have an infamously steep learning curve, but are very efficient when you get a handle on them.\n",
    "\n",
    "Our goal in this workshop is not to provide a deep (or even shallow) dive into regexes; instead, we want to expose you to them so that you're better prepared to do deep dives in the future.\n",
    "\n",
    "Regex testers are a useful tool in both understanding and creating regex expression. An example is this [website](https://regex101.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf750b5f-c08b-4f35-9eb8-b2182f892b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd9690d3-c12f-4a2c-bcc7-cfa3f3523949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test']\n",
      "This is a not a test.\n"
     ]
    }
   ],
   "source": [
    "test_string = 'This is a test.'\n",
    "# Find tokens\n",
    "tokens = re.findall(pattern=pattern, string=test_string)\n",
    "print(tokens)\n",
    "# Replace tokens\n",
    "replaced = re.sub(pattern=pattern, repl='not a test', string=test_string) \n",
    "print(replaced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d336ea3a-dd51-48a1-bf58-095298afdeb4",
   "metadata": {},
   "source": [
    "This is nice, but we could have done this somewhat easily with basic Python `string` functions. Let's try something more interesting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "244236dc-60b9-490c-98b7-33a6af5cb2b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'a', 'test']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word pattern matcher\n",
    "pattern = r'\\w+'\n",
    "re.findall(pattern, test_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e60923f-b981-4ad9-8777-0951105736d8",
   "metadata": {},
   "source": [
    "What did this do? Use the regex website to confirm your guess!\n",
    "\n",
    "For now, we won't go much further than this, but there are many resources online to continue learning about regexes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ebae34-7b23-49a9-8aad-8f49cb55f320",
   "metadata": {},
   "source": [
    "#### Lowercasing\n",
    "\n",
    "While there is often information in the \"casing\" of words (e.g., whether text is lowercase or uppercase), we often don't work in a regime where we're able to properly leverage this information. So, a common text cleaning step is to lowercase all text, in order to simplify our analysis.\n",
    "\n",
    "We can easily do this with the built-in string function `lower()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a07353a-0dfc-4438-ade1-d2fee7a3305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sowing_and_reaping_lower = sowing_and_reaping.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e2cab72-6002-4861-af38-d0b67296eb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I hear that John Andrews has given up his saloon; and a foolish thing\n",
      "it was. He was doing a splendid business. What could have induced him?\"\n",
      "\n",
      "\"They say that his wife was bitterly opposed to the busin\n",
      "------\n",
      "i hear that john andrews has given up his saloon; and a foolish thing\n",
      "it was. he was doing a splendid business. what could have induced him?\"\n",
      "\n",
      "\"they say that his wife was bitterly opposed to the busin\n"
     ]
    }
   ],
   "source": [
    "print(sowing_and_reaping[:200])\n",
    "print('------')\n",
    "print(sowing_and_reaping_lower[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39654022-c93b-4bf5-9a63-efaf2b8125c3",
   "metadata": {},
   "source": [
    "#### Removing Punctuation\n",
    "\n",
    "Sometimes, you might want to keep only the alphanumeric characters (i.e. the letters and numbers) and ditch the punctuation. This becomes less common when we consider more advanced NLP algorithms. In many cases, you may do this step *after* tokenization, which we will discuss in the next section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32e11d9c-8440-479e-86a0-13c7e1e42d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66e1da6f-0c1b-4588-a310-028c18fcad9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weve got quite a bit of punctuation here dont we Python DLab\n"
     ]
    }
   ],
   "source": [
    "punctuation_text = \"We've got quite a bit of punctuation here, don't we?!? #Python @D-Lab.\"\n",
    "no_punctuation = ''.join([char for char in punctuation_text if char not in punctuation])\n",
    "print(no_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2f9bba-e2a5-4839-b017-f979a09752a1",
   "metadata": {},
   "source": [
    "#### Stripping Blank Spaces\n",
    "\n",
    "Removing blank space is a common step, as we might come across text with extraneous blank space. This is particularly common when text is imported from messy places, like webpages.\n",
    "\n",
    "Python has a built-in function to deal with blank space on the *ends* of strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71d42f85-71fd-4fb3-9c29-c24c41188fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello!'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = ' Hello! '\n",
    "string.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f3d2ef-3111-43f4-b77d-08d9363a0756",
   "metadata": {},
   "source": [
    "What about within text? We will need to use a regular expression for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cca33c78-f8bf-438f-9737-f0a89c30c470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "This is a text file that has some extra blankspace at the start and end. Blankspace is a catch-all term for spaces, tabs, newlines, and a bunch of other things that computers distinguish but to us all look like spaces, tabs and newlines.\n",
      "\n",
      "\n",
      "The Python method called \"strip\" only catches blankspace at the start and end of a string. But it won't catch it in       the middle,\t\tfor example,\n",
      "\n",
      "in this sentence.\t\tOnce again, regular expressions will\n",
      "\n",
      "help\t\tus    with this.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example1_path = '../data/example1.txt'\n",
    "\n",
    "with open(example1_path, 'r') as file:\n",
    "    example1 = file.read()\n",
    "    \n",
    "print(example1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53e94cb2-49c1-4be2-8496-6dd124d90188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a text file that has some extra blankspace at the start and end. Blankspace is a catch-all term for spaces, tabs, newlines, and a bunch of other things that computers distinguish but to us all look like spaces, tabs and newlines.\n",
      "\n",
      "\n",
      "The Python method called \"strip\" only catches blankspace at the start and end of a string. But it won't catch it in       the middle,\t\tfor example,\n",
      "\n",
      "in this sentence.\t\tOnce again, regular expressions will\n",
      "\n",
      "help\t\tus    with this.\n"
     ]
    }
   ],
   "source": [
    "# Stripping only removes the ends\n",
    "print(example1.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "083b1553-dc87-4150-b857-76c2f0df8207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a text file that has some extra blankspace at the start and end. Blankspace is a catch-all term for spaces, tabs, newlines, and a bunch of other things that computers distinguish but to us all look like spaces, tabs and newlines. The Python method called \"strip\" only catches blankspace at the start and end of a string. But it won\\'t catch it in the middle, for example, in this sentence. Once again, regular expressions will help us with this.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A regular expression will handle blank spaces within the text\n",
    "blankspace_pattern = r'\\s+'\n",
    "blankspace_repl = ' '\n",
    "clean_text = re.sub(blankspace_pattern, blankspace_repl, example1)\n",
    "clean_text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2a168e-e2e0-46c0-b794-fa0ecae400dc",
   "metadata": {},
   "source": [
    "#### Removing URLs, Hashtags, and Numbers\n",
    "\n",
    "Text containing non-alphabetic symbols may have additional meaning beyond simply using punctuation or numbers. For example, text may contain URLs, hashtags, or numbers. Each of these are informative in their own right.\n",
    "\n",
    "However, we rarely care about the exact URL used in a tweet. Similarly, we might not care about specific hashtags, or the precise number used. While, we could remove them completely, it's often informative to know that there *exists* a URL, hashtag, or number.\n",
    "\n",
    "So, we replace individual URLs, hashtags, and numbers with a \"symbol\" that preserves the fact these structures exist in the text. It's standard to just use the strings \"URL\", \"HASHTAG\", and \"DIGIT\".\n",
    "\n",
    "Since these types of text often contain precise structure, they're an apt case for using regular expressions. Let's apply these patterns to the Tweets above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8069337-4a39-4437-8710-f9b5c58335f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@VirginAmerica @virginmedia I'm flying your #fabulous #Seductive skies again! U take all the #stress away from travel http://t.co/ahlXHhKiyn\n"
     ]
    }
   ],
   "source": [
    "# Get a Tweet with a URL in it\n",
    "url_tweet = tweets['text'].iloc[13]\n",
    "print(url_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc1fa10d-e546-4e61-89e5-adf53c7e6262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@VirginAmerica @virginmedia I'm flying your #fabulous #Seductive skies again! U take all the #stress away from travel  URL \""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# URL \n",
    "url_pattern = r'(http|ftp|https):\\/\\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\\/~+#-]*[\\w@?^=%&\\/~+#-])'\n",
    "url_repl = ' URL '\n",
    "re.sub(url_pattern, url_repl, url_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40416f9e-4052-425f-9a00-94a3b0a94e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@VirginAmerica @virginmedia I'm flying your HASHTAG  HASHTAG  skies again! U take all the HASHTAG  away from travel http://t.co/ahlXHhKiyn\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hashtag\n",
    "hashtag_pattern = r'(?:^|\\s)[ï¼ƒ#]{1}(\\w+)'\n",
    "hashtag_repl = ' HASHTAG '\n",
    "re.sub(hashtag_pattern, hashtag_repl, url_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fba96052-a24e-46ca-8d99-60e47b485c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@VirginAmerica help, left expensive headphones on flight 89 IAD to LAX today. Seat 2A. No one answering L&amp;F number at LAX!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'@VirginAmerica help, left expensive headphones on flight  DIGIT  IAD to LAX today. Seat  DIGIT A. No one answering L&amp;F number at LAX!'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Digits\n",
    "digit_tweet = tweets['text'].iloc[32]\n",
    "print(digit_tweet)\n",
    "digit_pattern = '\\d+'\n",
    "digit_repl = ' DIGIT '\n",
    "re.sub(digit_pattern, digit_repl, digit_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a288b9a-a33a-48be-bac4-93ffd39318ec",
   "metadata": {},
   "source": [
    "What other kinds of text strings can you think of that we might want to replace?\n",
    "\n",
    "Natural language is complex, and so there may be use cases where we might need specialized packages for preprocessing or removing text. For example, the [`emoji` package](https://pypi.org/project/emoji/) may be useful for social media text. The [`textacy` package](https://textacy.readthedocs.io/en/latest/) also provides useful preprocessing tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f5e64f-c6a0-447b-973e-a48e9eb80ade",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Challenge 3: Text Cleaning with Multiple Steps\n",
    "\n",
    "In Challenge 1, we imported many Amazon reviews, and stored them in a variable called `reviews`. Each element of the list is a string, representing the text of a single review. For each review:\n",
    "\n",
    "* Replace any URLs and digits.\n",
    "* Make all characters lower case.\n",
    "* Strip all blankspace.\n",
    "\n",
    "Keep in mind: the order in which you do these steps matters!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2d599284-2392-488c-a4a8-dad5eeb50f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    # YOUR CODE HERE\n",
    "    \"\"\"Preprocesses a string.\"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Replace URLs\n",
    "    url_pattern = r'https?:\\/\\/.*[\\r\\n]*'\n",
    "    url_repl = ' URL '\n",
    "    text = re.sub(url_pattern, url_repl, text)\n",
    "    # Replace digits\n",
    "    digit_pattern = '\\d+'\n",
    "    digit_repl = ' DIGIT '\n",
    "    text = re.sub(digit_pattern, digit_repl, text)\n",
    "    # Remove blank spaces\n",
    "    blankspace_pattern = r'\\s+'\n",
    "    blankspace_repl = ' '\n",
    "    text = re.sub(blankspace_pattern, blankspace_repl, text)\n",
    "    # Strip\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "318013a3-de59-477d-8458-74fc01a10319",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_prepocessed = [preprocess(review) for review in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0996af6f-8e95-44c7-8116-fd01605b4c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have a DIGIT week old... he had gas and constipation problems for the first DIGIT weeks. we tried two different kinds of similac including for fussiness and gas and neither seemed to work. we switched to the organic a few weeks ago and saw quick improvement. i wish i could breast feed but i'm unable to, so for now this seems the best option especially since it was recommended we stick with a ready made formula for the gas problems.<br />ive read a lot of the reviews and took into consideration the information about sucrose. i plan on talking to the pediatrician and my midwife for additional information beyond the article written about it, especially since that is from DIGIT . i realize the concern and i am doing research on making my own formula so i know exactly whats in it and that its organic, but in the mean time baby l eats great with this, is healthy, and has fewer stomach problems. it's middle of the road when it comes to $ - although amazn is one of the more expensive places!!! target has the best price. so for now it works and i recommend it!!\n"
     ]
    }
   ],
   "source": [
    "print(reviews_prepocessed[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacc7c15-1bc7-47cd-b668-ab57634c8b04",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "One of the most important steps in text analysis is tokenization. This is the process of breaking down the text into \"tokens\", which are distinct chunks that we recognize as unique in whatever corpus we're working in.\n",
    "\n",
    "Let's start by importing an example file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3cd9d972-ae66-4415-8ce6-359cb9db0329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this little example, we're going to see some of the problems that regularly appear in tokenization. Tokenization may seem simple, but it's harder than it first appears. Why is it so hard? Punctuations, contractions (like don't, won't and would've) get in the way. What do you do when you have #hashtags, @TwitterHandles, or https://urls.com? Different packages will make different decisions on when to split text apart, and when not to.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example2_path = '../data/example2.txt'\n",
    "\n",
    "with open(example2_path) as file:\n",
    "    example2 = file.read()\n",
    "    \n",
    "print(example2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b402ed3-de7f-4e16-ade6-444c9e74f8eb",
   "metadata": {},
   "source": [
    "Let's try naively tokenizing by splitting up the text according to blankspace, using a basic Python string method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "99fabb62-ff4e-48f7-b057-f72983dcf87e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'this',\n",
       " 'little',\n",
       " 'example,',\n",
       " \"we're\",\n",
       " 'going',\n",
       " 'to',\n",
       " 'see',\n",
       " 'some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'problems',\n",
       " 'that',\n",
       " 'regularly',\n",
       " 'appear',\n",
       " 'in',\n",
       " 'tokenization.',\n",
       " 'Tokenization',\n",
       " 'may',\n",
       " 'seem']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = example2.split()\n",
    "# Print first ten tokens\n",
    "tokens[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aa6b33-e927-4adc-8d6b-9a5a1f310e62",
   "metadata": {},
   "source": [
    "We can roughly think of this as \"word tokenization\". However, it's not always clear that simply splitting up by spaces will get what we want. Consider contractions, for example, which really consist of two words connected together. More advanced tokenizations will actually treat these words differently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6263bf1f-b033-49c6-81b7-794d0e5bf1cb",
   "metadata": {},
   "source": [
    "`nltk` has a function called `word_tokenize` which can tokenize a string for us in an intelligent fashion. Ultimately, `nltk` basically is a bunch of regular expressions under the hood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "06c63c1d-e1b3-4f21-aa57-460771e60801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/blueraspberry/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3f7b4d1a-43ab-48e1-8bd0-eef90d57a6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "nltk_tokens = word_tokenize(example2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9f8769c1-6446-465b-a9b3-06e26086a330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'this', 'little', 'example', ',', 'we', \"'re\", 'going', 'to', 'see', 'some', 'of', 'the', 'problems', 'that', 'regularly', 'appear', 'in', 'tokenization', '.', 'Tokenization', 'may', 'seem', 'simple', ',', 'but', 'it', \"'s\", 'harder', 'than', 'it', 'first', 'appears', '.', 'Why', 'is', 'it', 'so', 'hard', '?', 'Punctuations', ',', 'contractions', '(', 'like', 'do', \"n't\", ',', 'wo', \"n't\", 'and', 'would', \"'ve\", ')', 'get', 'in', 'the', 'way', '.', 'What', 'do', 'you', 'do', 'when', 'you', 'have', '#', 'hashtags', ',', '@', 'TwitterHandles', ',', 'or', 'https', ':', '//urls.com', '?', 'Different', 'packages', 'will', 'make', 'different', 'decisions', 'on', 'when', 'to', 'split', 'text', 'apart', ',', 'and', 'when', 'not', 'to', '.']\n"
     ]
    }
   ],
   "source": [
    "print(nltk_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9022de3b-ab72-4616-bba2-831f58491757",
   "metadata": {},
   "source": [
    "Looking at this example, you can see how `nltk` has made certain decisions about where and when to tokenize. Tokenization is critical for downstream processing, and there's a variety of methods for performing the tokenizing. Let's take a look at `spaCy`'s tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "12cdd905-96ef-42f4-852f-a39ceede2a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (3.5.3)\n",
      "Requirement already satisfied: jinja2 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from spacy) (2.4.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: setuptools in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from spacy) (65.6.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from spacy) (1.10.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from spacy) (22.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from spacy) (1.23.5)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from spacy) (8.1.10)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.14)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from jinja2->spacy) (2.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install spaCy if necessary\n",
    "%pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "59e4dbd6-46fa-477b-af86-63913670123e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from en-core-web-sm==3.5.0) (3.5.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.23.5)\n",
      "Requirement already satisfied: setuptools in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (65.6.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.64.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (5.2.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (22.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.9)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: jinja2 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.1)\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "07f98810-7494-48bf-9d01-0c9df9bfc562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spaCy and load the dictionary\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Pass the example into the English pipeline\n",
    "doc = nlp(example2)\n",
    "spacy_tokens = [token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e8acb52a-e49b-4fdd-b09c-2e6c4fd5d7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'this', 'little', 'example', ',', 'we', \"'re\", 'going', 'to', 'see', 'some', 'of', 'the', 'problems', 'that', 'regularly', 'appear', 'in', 'tokenization', '.', 'Tokenization', 'may', 'seem', 'simple', ',', 'but', 'it', \"'s\", 'harder', 'than', 'it', 'first', 'appears', '.', 'Why', 'is', 'it', 'so', 'hard', '?', 'Punctuations', ',', 'contractions', '(', 'like', 'do', \"n't\", ',', 'wo', \"n't\", 'and', 'would', \"'ve\", ')', 'get', 'in', 'the', 'way', '.', 'What', 'do', 'you', 'do', 'when', 'you', 'have', '#', 'hashtags', ',', '@', 'TwitterHandles', ',', 'or', 'https', ':', '//urls.com', '?', 'Different', 'packages', 'will', 'make', 'different', 'decisions', 'on', 'when', 'to', 'split', 'text', 'apart', ',', 'and', 'when', 'not', 'to', '.']\n",
      "['In', 'this', 'little', 'example', ',', 'we', \"'re\", 'going', 'to', 'see', 'some', 'of', 'the', 'problems', 'that', 'regularly', 'appear', 'in', 'tokenization', '.', 'Tokenization', 'may', 'seem', 'simple', ',', 'but', 'it', \"'s\", 'harder', 'than', 'it', 'first', 'appears', '.', 'Why', 'is', 'it', 'so', 'hard', '?', 'Punctuations', ',', 'contractions', '(', 'like', 'do', \"n't\", ',', 'wo', \"n't\", 'and', 'would', \"'ve\", ')', 'get', 'in', 'the', 'way', '.', 'What', 'do', 'you', 'do', 'when', 'you', 'have', '#', 'hashtags', ',', '@TwitterHandles', ',', 'or', 'https://urls.com', '?', 'Different', 'packages', 'will', 'make', 'different', 'decisions', 'on', 'when', 'to', 'split', 'text', 'apart', ',', 'and', 'when', 'not', 'to', '.', '\\n']\n"
     ]
    }
   ],
   "source": [
    "# Compare NLTK to spaCy\n",
    "print(nltk_tokens)\n",
    "print(spacy_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e2147b-841e-4a1f-97f1-fd64f23eb118",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Challenge 4: Tokenizing a Large Text\n",
    "\n",
    "Tokenize \"Sowing and Reaping\", which we imported at the beginning of this workshop. Use a method of your choice.\n",
    "\n",
    "Once you've tokenized, find all the unique words types (you might want the `set` function). Then, sort the resulting `set` object to create a vocabulary (you might want to use the `sorted` function).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b1678051-ecc8-4210-9547-4884598a048a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'hear', 'that', 'John', 'Andrews', 'has', 'given', 'up', 'his', 'saloon', ';', 'and', 'a', 'foolish', 'thing', 'it', 'was', '.', 'He', 'was', 'doing', 'a', 'splendid', 'business', '.', 'What', 'could', 'have', 'induced', 'him', '?', \"''\", '``', 'They', 'say', 'that', 'his', 'wife', 'was', 'bitterly', 'opposed', 'to', 'the', 'business', '.', 'I', \"don't\", 'know', ',', 'but', 'I', 'think', 'it', 'quite', 'likely', '.', 'She', 'has', 'never', 'seemed', 'happy', 'since', 'John', 'has', 'kept', 'saloon', '.', \"''\", '``', 'Well', ',', 'I', 'would', 'never', 'let', 'any', 'woman', 'lead', 'me', 'by', 'the', 'nose', '.', 'I', 'would', 'let', 'her', 'know', 'that', 'as', 'the', 'living', 'comes', 'by', 'me', ',', 'the', 'way', 'of', 'getting']\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# nltk\n",
    "nltk_sowing = word_tokenize(sowing_and_reaping)\n",
    "print(nltk_sowing[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "54534480-fb19-4c0e-9528-b94e30b7b284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'hear', 'that', 'John', 'Andrews', 'has', 'given', 'up', 'his', 'saloon', ';', 'and', 'a', 'foolish', 'thing', '\\n', 'it', 'was', '.', 'He', 'was', 'doing', 'a', 'splendid', 'business', '.', 'What', 'could', 'have', 'induced', 'him', '?', '\"', '\\n\\n', '\"', 'They', 'say', 'that', 'his', 'wife', 'was', 'bitterly', 'opposed', 'to', 'the', 'business', '.', 'I', 'do', \"n't\", '\\n', 'know', ',', 'but', 'I', 'think', 'it', 'quite', 'likely', '.', 'She', 'has', 'never', 'seemed', 'happy', 'since', 'John', '\\n', 'has', 'kept', 'saloon', '.', '\"', '\\n\\n', '\"', 'Well', ',', 'I', 'would', 'never', 'let', 'any', 'woman', 'lead', 'me', 'by', 'the', 'nose', '.', 'I', 'would', 'let', 'her', '\\n', 'know', 'that', 'as', 'the', 'living', 'comes']\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Pass the example into the English pipeline\n",
    "spacy_sowing = nlp(sowing_and_reaping)\n",
    "spacy_tokens = [token.text for token in spacy_sowing]\n",
    "print(spacy_tokens[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ecb63546-3a2e-4825-91e1-6bb39d287c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# nltk: unique words with set\n",
    "unique_nltk = set(nltk_sowing)\n",
    "sorted_nltk = sorted(unique_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "73fce5ba-9a10-44ab-96f4-8474cb7277c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# spacy: unique words with set\n",
    "unique_spacy = set(spacy_tokens)\n",
    "sorted_spacy = sorted(unique_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "61c6d45b-35b6-4960-a584-8882ef712d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '#', '$', '%', '&', \"'\", \"''\", \"'AS-IS\", \"'Do\", \"'He\", \"'Here\", \"'Hurrah\", \"'Mary\", \"'My\", \"'One\", \"'Paul\", \"'That\", \"'What\", \"'Young\", \"'and\", \"'bout\", \"'business\", \"'d\", \"'fail\", \"'is\", \"'ll\", \"'one\", \"'our\", \"'re\", \"'right\", \"'s\", \"'spec\", \"'the\", \"'ve\", '(', ')', '*', ',', '-', '--', '.', '._', '//gutenberg.net/license', '//pglaf.org', '//pglaf.org/donate', '//pglaf.org/fundraising', '//www.gutenberg.net', '//www.gutenberg.net/1/0/2/3/10234', '//www.gutenberg.net/1/1/0/2/11022', '//www.gutenberg.net/2/4/6/8/24689', '//www.gutenberg.net/GUTINDEX.ALL', '//www.ibiblio.org/gutenberg/etext06', '//www.pglaf.org', '/etext', '00', '01', '02', '03', '04', '05', '1', '1.A', '1.B', '1.C', '1.D', '1.E', '1.E.1', '1.E.2', '1.E.3', '1.E.4', '1.E.5', '1.E.6', '1.E.7', '1.E.8', '1.E.9', '1.F', '1.F.1', '1.F.2', '1.F.3', '1.F.4', '1.F.5', '1.F.6', '10', '10000', '10234', '11022.txt', '11022.zip', '1500', '16th', '2', '20', '200', '2001', '2003', '24689', '25', '3', '30', '4', '4557']\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# nltk: unique words with set\n",
    "print(sorted(unique_nltk)[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4a13d71f-8d40-4670-b2a3-233f72438d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', '\\n\\n', '\\n\\n\\n', '\\n\\n\\n\\n', '\\n\\n\\n\\n\\n', '\\n\\n ', '\\n\\n  ', '\\n\\n    ', '\\n\\n       ', '\\n  ', '\\n   ', '\\n     ', ' ', '      ', '!', '\"', '\"[3', '\"[6', '#', '$', '%', '&', \"'\", \"'bout\", \"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '(', ')', '*', ',', '-', '--', '.', '/', '/etext', '00', '01', '02', '03', '04', '05', '1', '1.A.', '1.B.', '1.C', '1.C.', '1.D.', '1.E', '1.E.', '1.E.1', '1.E.2', '1.E.3', '1.E.4', '1.E.5', '1.E.6', '1.E.7', '1.E.8', '1.E.9', '1.F.', '1.F.1', '1.F.2', '1.F.3', '1.F.4', '1.F.5', '1.F.6', '10', '10000', '10234', '11022.txt', '11022.zip', '1500', '16th', '1887', '2', '20', '200', '2001', '2003', '24689', '25', '3', '30', '4', '4557', '5', '5,000', '50', '500', '501(c)(3', '596', '6', '60', '6221541', '64', '7', '8', '801']\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# spacy: unique words with set\n",
    "print(sorted(sorted_spacy)[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "60d242cb-4d94-4ed3-a379-d1962e98a47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wifely', 'wild', 'wilderness', 'will', 'willed', 'willing', 'win', 'wind', 'winds', 'wine', 'wings', 'wins', 'winter', 'wise', 'wisely', 'wiser', 'wish', 'wished', 'wishing', 'with', 'within', 'without', 'witness', 'witnessed', 'witnesses', 'witty', 'wives', 'wizard', 'wo', 'wolf', 'woman', \"woman's\", 'womanhood', 'womanly', 'women', 'won', 'wonder', 'wonderful', 'woods', 'woolen', 'word', 'words', 'wore', 'work', 'worked', 'working', 'works', 'world', \"world's\", 'worldly', 'worm', 'worn', 'worrying', 'worse', 'worst', 'worth', 'worthless', 'worthy', 'would', \"wouldn't\", 'wound', 'wove', 'woven', 'wrath', 'wreath', 'wreck', 'wrecked', 'wrecks', 'wretch', 'wretched', 'wretchedness', 'wrinkles', 'write', 'writer', 'writes', 'writing', 'written', 'wrong', 'wronged', 'wrote', 'www.gutenberg.net', 'year', 'yearned', 'years', 'yes', 'yesterday', 'yet', 'yield', 'yielded', 'yielding', 'you', 'young', 'younger', 'your', 'yours', 'yourself', 'yourselves', 'youth', 'zealous', 'zipped']\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# nltk: unique words with set\n",
    "print(sorted(unique_nltk)[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5f883c2e-687e-4e82-b5ca-54392ff1fdce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wifely', 'wild', 'wilderness', 'will', 'willed', 'willing', 'win', 'wind', 'winds', 'wine', 'wings', 'wins', 'winter', 'wise', 'wisely', 'wiser', 'wish', 'wished', 'wishing', 'with', 'within', 'without', 'witness', 'witnessed', 'witnesses', 'witty', 'wives', 'wizard', 'wo', 'wolf', 'woman', 'womanhood', 'womanly', 'women', 'won', 'wonder', 'wonderful', 'woods', 'woolen', 'word', 'words', 'words--_getting', 'wore', 'work', 'worked', 'working', 'works', 'world', 'worldly', 'worm', 'worn', 'worrying', 'worse', 'worst', 'worth', 'worthless', 'worthy', 'would', 'wound', 'wove', 'woven', 'wrath', 'wreath', 'wreck', 'wrecked', 'wrecks', 'wretch', 'wretched', 'wretchedness', 'wrinkles', 'write', 'writer', 'writes', 'writing', 'written', 'wrong', 'wronged', 'wrote', 'www.gutenberg.net', 'year', 'yearned', 'years', 'yes', 'yesterday', 'yet', 'yield', 'yielded', 'yielding', 'you', 'you--', 'you[r', 'young', 'younger', 'your', 'yours', 'yourself', 'yourselves', 'youth', 'zealous', 'zipped']\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# spacy: unique words with set\n",
    "print(sorted(sorted_spacy)[-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61259cad-3b35-4ce9-89b3-77ba8283bc18",
   "metadata": {},
   "source": [
    "## Removing Stop Words\n",
    "\n",
    "Text often has words that are very common and usually not informative. These words tend to be pronouns or articles, such as \"the\", \"a\", \"it\", \"them\", etc. In many cases, these \"stop words\" are those that we may wish to remove before performing computation since they usually are not very informative. \n",
    "\n",
    "In practice, this is simple to do - we just filter out tokens by words. However, we may want to use different \"stop word lists\", depending on our use case. For example, `nltk` has a stop word list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7989a296-4f9c-44eb-b0f1-c554194fc7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6bf9ff5f-73c8-446c-be8e-dabe1bd6d8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be']\n"
     ]
    }
   ],
   "source": [
    "# What kinds of words are in the list?\n",
    "print(stop[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c49a372c-ecef-412f-bdf3-34f76f8b3f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'little', 'example,', \"we're\", 'going', 'see', 'problems', 'regularly', 'appear', 'tokenization.', 'Tokenization', 'may', 'seem', 'simple,', 'harder', 'first', 'appears.', 'Why', 'hard?', 'Punctuations,', 'contractions', '(like', \"don't,\", \"would've)\", 'get', 'way.', 'What', '#hashtags,', '@TwitterHandles,', 'https://urls.com?', 'Different', 'packages', 'make', 'different', 'decisions', 'split', 'text', 'apart,', 'to.']\n"
     ]
    }
   ],
   "source": [
    "# Remove tokens that are stop words\n",
    "tokens = [token for token in tokens if token not in stop]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "95d66cdf-89a4-49b6-812d-7215f97272d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this little example, we're going to see some of the problems that regularly appear in tokenization. Tokenization may seem simple, but it's harder than it first appears. Why is it so hard? Punctuations, contractions (like don't, won't and would've) get in the way. What do you do when you have #hashtags, @TwitterHandles, or https://urls.com? Different packages will make different decisions on when to split text apart, and when not to.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare to the original text\n",
    "print(example2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7692ad-8545-435f-b0cd-80f13ffff209",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization\n",
    "\n",
    "Stemming and lemmatization both refer to removing morphological affixes on words. Many words consist of a \"core\" word with a modified ending that adjusts the word's meaning in a given context. For example, the word \"grows\" is simply \"grow\" with an \"s\" added to denote a change in verb tense. In many cases, we're interested in the core content of the word. Stemming and lemmatization are the process of getting at the \"core\" of a word. This \"core\" component is often referred to as the *lemma*.\n",
    "\n",
    "Stemming is a rudimentary approach to obtaining the lemma: it simply removes an ending of a word. So, \"grows\" would be stemmed to \"grow\". The word \"running\" would be stemmed to \"run\".\n",
    "\n",
    "Lemmatization is more general: it aims to find the lemma of a word, but can handle cases where stemming may not work. For example, the word \"fairies\" cannot be stemmed to the lemma, \"fairy\". So, we need additional rules - provided by lemmatization - that can appropriately turn \"fairies\" into \"fairy\".\n",
    "\n",
    "`nltk` provides many algorithms for stemming. We'll use the Snowball Stemmer, which we'll import from `nltk`. We'll also look at the Word Net Lemmatizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "174b304d-5d5b-4303-9262-c43f6c6948e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "02fd8fda-60f5-4b1b-9e53-2b0aa4fee43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/blueraspberry/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6e9e5f2e-2384-4631-b260-8fbe1e6970cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the stemmer and lemmatizer\n",
    "stemmer = SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dac5719f-83d2-49ef-b1a6-3c9cfe0a7ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grow\n",
      "run\n",
      "code\n"
     ]
    }
   ],
   "source": [
    "# Stemming examples\n",
    "print(stemmer.stem('grows'))\n",
    "print(stemmer.stem('running'))\n",
    "print(stemmer.stem('coded'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "35604e03-b724-4896-85be-7a664d18c27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairi\n",
      "wolv\n",
      "abaci\n",
      "leav\n",
      "carri\n"
     ]
    }
   ],
   "source": [
    "# When does stemming not quite work?\n",
    "print(stemmer.stem('fairies'))\n",
    "print(stemmer.stem('wolves'))\n",
    "print(stemmer.stem('abaci'))\n",
    "print(stemmer.stem('leaves'))\n",
    "print(stemmer.stem('carried'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7d46f1f8-e6aa-48db-a954-508fdf4f542a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairy\n",
      "wolf\n",
      "abacus\n",
      "leaf\n",
      "carried\n"
     ]
    }
   ],
   "source": [
    "# Let's try lemmatizing these, instead:\n",
    "print(lemmatizer.lemmatize('fairies'))\n",
    "print(lemmatizer.lemmatize('wolves'))\n",
    "print(lemmatizer.lemmatize('abaci'))\n",
    "print(lemmatizer.lemmatize('leaves'))\n",
    "print(lemmatizer.lemmatize('carried'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044da5ed-cf6a-4114-b4b7-102e8504ce42",
   "metadata": {},
   "source": [
    "What happened with that last one? Sometimes we need to provide the lemmatizer a 'part-of-speech' tag to help resolve ambiguous cases. This is another argument in the lemmatizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "50976d9b-5151-4dc1-862d-6cb2576c8dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carry\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize('carried', pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d6c43f-33e9-4f48-8812-f77e31a85111",
   "metadata": {},
   "source": [
    "Try it with \"leaves\", which has more than one way to lemmatize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "961a0ed8-d653-41c6-932d-90dba75329d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leaf\n",
      "leave\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize('leaves', pos='n'))\n",
    "print(lemmatizer.lemmatize('leaves', pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2a4cf0-cfd9-4116-94ef-58ef4426b482",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Challenge 5: Apply a Lemmatizer to Text\n",
    "\n",
    "Lemmatize the tokenized `example2` text using the `nltk`'s `WordNetLemmatizer`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dc687676-4298-4a88-9655-2e77b72b4192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "example2_token = word_tokenize(example2)\n",
    "example2_lemma = [lemmatizer.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d6f9a0dc-87e9-4b2c-bfea-b55ea8567ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'little', 'example,', \"we're\", 'going', 'see', 'problem', 'regularly', 'appear', 'tokenization.', 'Tokenization', 'may', 'seem', 'simple,', 'harder', 'first', 'appears.', 'Why', 'hard?', 'Punctuations,', 'contraction', '(like', \"don't,\", \"would've)\", 'get', 'way.', 'What', '#hashtags,', '@TwitterHandles,', 'https://urls.com?', 'Different', 'package', 'make', 'different', 'decision', 'split', 'text', 'apart,', 'to.']\n"
     ]
    }
   ],
   "source": [
    "print(example2_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "86e9ad80-47cb-444b-a2bb-e2d75e52d2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this little example, we're going to see some of the problems that regularly appear in tokenization. Tokenization may seem simple, but it's harder than it first appears. Why is it so hard? Punctuations, contractions (like don't, won't and would've) get in the way. What do you do when you have #hashtags, @TwitterHandles, or https://urls.com? Different packages will make different decisions on when to split text apart, and when not to.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(example2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a429b117-1983-45bf-bdff-d6f0f83a8c22",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "### Challenge 6: Putting it All Together\n",
    "\n",
    "Write a function called `preprocess()` that accepts a string and performs the following preprocessing steps:\n",
    "\n",
    "* Lowercase text.\n",
    "* Replace all URLs and numbers with their respective tokens.\n",
    "* Strip blankspace.\n",
    "* Tokenize.\n",
    "* Remove punctuation.\n",
    "* Remove stop words.\n",
    "* Lemmatize the tokens.\n",
    "\n",
    "Apply this function to `sowing_and_reaping`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4f5dbfb0-97b1-4abf-845c-17a0d4942831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    # YOUR CODE HERE\n",
    "    \"\"\"Preprocesses a string.\"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Replace URLs\n",
    "    url_pattern = r'(http|ftp|https):\\/\\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\\/~+#-]*[\\w@?^=%&\\/~+#-])'\n",
    "    url_repl = ' URL '\n",
    "    text = re.sub(url_pattern, url_repl, text)\n",
    "    # Replace digits\n",
    "    digit_pattern = '\\d+'\n",
    "    digit_repl = ' DIGIT '\n",
    "    text = re.sub(digit_pattern, digit_repl, text)\n",
    "    # Remove blank spaces\n",
    "    blankspace_pattern = r'\\s+'\n",
    "    blankspace_repl = ' '\n",
    "    text = re.sub(blankspace_pattern, blankspace_repl, text).strip()\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove punctuation\n",
    "    tokens = [token for token in tokens if token not in punctuation]\n",
    "    # Remove stop words\n",
    "    stop = stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stop]\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8d259147-ad68-417c-99a6-1fecfb9b9a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hear',\n",
       " 'john',\n",
       " 'andrew',\n",
       " 'given',\n",
       " 'saloon',\n",
       " 'foolish',\n",
       " 'thing',\n",
       " 'splendid',\n",
       " 'business',\n",
       " 'could',\n",
       " 'induced',\n",
       " \"''\",\n",
       " '``',\n",
       " 'say',\n",
       " 'wife',\n",
       " 'bitterly',\n",
       " 'opposed',\n",
       " 'business',\n",
       " \"n't\",\n",
       " 'know',\n",
       " 'think',\n",
       " 'quite',\n",
       " 'likely',\n",
       " 'never',\n",
       " 'seemed',\n",
       " 'happy',\n",
       " 'since',\n",
       " 'john',\n",
       " 'kept',\n",
       " 'saloon',\n",
       " \"''\",\n",
       " '``',\n",
       " 'well',\n",
       " 'would',\n",
       " 'never',\n",
       " 'let',\n",
       " 'woman',\n",
       " 'lead',\n",
       " 'nose',\n",
       " 'would',\n",
       " 'let',\n",
       " 'know',\n",
       " 'living',\n",
       " 'come',\n",
       " 'way',\n",
       " 'getting',\n",
       " 'affair',\n",
       " 'long',\n",
       " 'well',\n",
       " 'provided',\n",
       " \"''\",\n",
       " '``']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(sowing_and_reaping[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca49653f-8700-4681-a50a-43bbb14c124d",
   "metadata": {},
   "source": [
    "## Powerful Features of `spaCy`\n",
    "\n",
    "We will end this portion of the workshop by examining some of the more powerful features offered by the newer NLP library, `spaCy`. Beside being quite fast, `spaCy` provides very powerful built-in tools in its tokenizer. For example, we automatically get many of the above operations in one fell swoop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f741be70-2e03-4b95-b1a0-d5a525da9ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: We; Lemma: we; Part-of-speech: PRON; Token shape: Xx; Alphabetical? True; Stop Word? True\n",
      "Token: 're; Lemma: be; Part-of-speech: AUX; Token shape: 'xx; Alphabetical? False; Stop Word? True\n",
      "Token: learning; Lemma: learn; Part-of-speech: VERB; Token shape: xxxx; Alphabetical? True; Stop Word? False\n",
      "Token: about; Lemma: about; Part-of-speech: ADP; Token shape: xxxx; Alphabetical? True; Stop Word? True\n",
      "Token: natural; Lemma: natural; Part-of-speech: ADJ; Token shape: xxxx; Alphabetical? True; Stop Word? False\n",
      "Token: language; Lemma: language; Part-of-speech: NOUN; Token shape: xxxx; Alphabetical? True; Stop Word? False\n",
      "Token: processing; Lemma: processing; Part-of-speech: NOUN; Token shape: xxxx; Alphabetical? True; Stop Word? False\n",
      "Token: at; Lemma: at; Part-of-speech: ADP; Token shape: xx; Alphabetical? True; Stop Word? True\n",
      "Token: Berkeley; Lemma: Berkeley; Part-of-speech: PROPN; Token shape: Xxxxx; Alphabetical? True; Stop Word? False\n",
      "Token: .; Lemma: .; Part-of-speech: PUNCT; Token shape: .; Alphabetical? False; Stop Word? False\n"
     ]
    }
   ],
   "source": [
    "short_example = \"We're learning about natural language processing at Berkeley.\"\n",
    "doc = nlp(short_example)\n",
    "\n",
    "for token in doc:\n",
    "    print(\n",
    "        f\"Token: {token.text}; Lemma: {token.lemma_}; Part-of-speech: {token.pos_}; \"\n",
    "        f\"Token shape: {token.shape_}; Alphabetical? {token.is_alpha}; Stop Word? {token.is_stop}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcec4cf7-eb3c-4b4a-9008-bb2c113634c8",
   "metadata": {},
   "source": [
    "Tokenizing, lemmatization, part of speech tagging, stop word detection, and a couple other things are provided to us up front when we pass a text into the `nlp` module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c70884-da79-45e8-a3b1-905f57241817",
   "metadata": {},
   "source": [
    "`spaCy` also comes with some pretty shiny visualization tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ab7351a4-b05f-4938-894f-9a21309b3473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"4bfdc3bfa00c485d89386916eda46ef0-0\" class=\"displacy\" width=\"1400\" height=\"287.0\" direction=\"ltr\" style=\"max-width: none; height: 287.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">We</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"200\">'re</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"200\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">learning</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">about</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">natural</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"800\">language</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"800\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"950\">processing</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"950\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">at</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1250\">Berkeley.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1250\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4bfdc3bfa00c485d89386916eda46ef0-0-0\" stroke-width=\"2px\" d=\"M62,152.0 62,102.0 350.0,102.0 350.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4bfdc3bfa00c485d89386916eda46ef0-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M62,154.0 L58,146.0 66,146.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4bfdc3bfa00c485d89386916eda46ef0-0-1\" stroke-width=\"2px\" d=\"M212,152.0 212,127.0 347.0,127.0 347.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4bfdc3bfa00c485d89386916eda46ef0-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M212,154.0 L208,146.0 216,146.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4bfdc3bfa00c485d89386916eda46ef0-0-2\" stroke-width=\"2px\" d=\"M362,152.0 362,127.0 497.0,127.0 497.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4bfdc3bfa00c485d89386916eda46ef0-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M497.0,154.0 L501.0,146.0 493.0,146.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4bfdc3bfa00c485d89386916eda46ef0-0-3\" stroke-width=\"2px\" d=\"M662,152.0 662,127.0 797.0,127.0 797.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4bfdc3bfa00c485d89386916eda46ef0-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M662,154.0 L658,146.0 666,146.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4bfdc3bfa00c485d89386916eda46ef0-0-4\" stroke-width=\"2px\" d=\"M812,152.0 812,127.0 947.0,127.0 947.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4bfdc3bfa00c485d89386916eda46ef0-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M812,154.0 L808,146.0 816,146.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4bfdc3bfa00c485d89386916eda46ef0-0-5\" stroke-width=\"2px\" d=\"M512,152.0 512,102.0 950.0,102.0 950.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4bfdc3bfa00c485d89386916eda46ef0-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M950.0,154.0 L954.0,146.0 946.0,146.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4bfdc3bfa00c485d89386916eda46ef0-0-6\" stroke-width=\"2px\" d=\"M962,152.0 962,127.0 1097.0,127.0 1097.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4bfdc3bfa00c485d89386916eda46ef0-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1097.0,154.0 L1101.0,146.0 1093.0,146.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4bfdc3bfa00c485d89386916eda46ef0-0-7\" stroke-width=\"2px\" d=\"M1112,152.0 1112,127.0 1247.0,127.0 1247.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4bfdc3bfa00c485d89386916eda46ef0-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1247.0,154.0 L1251.0,146.0 1243.0,146.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"dep\", options={'compact': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f9e25d-7f90-4656-804e-351af0cf0356",
   "metadata": {},
   "source": [
    "For longer texts, we also get the ability to perform a variety of other operations very easily. Here are some cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "65eee367-1ade-4b95-b5bb-21968a9050b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "example3_path = '../data/example3.txt'\n",
    "\n",
    "with open(example3_path, 'r') as file:\n",
    "    example3 = file.read()\n",
    "    \n",
    "doc = nlp(example3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "64d59d76-d30c-44fb-b862-c208c3212e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D-Lab helps Berkeley faculty, staff, and students move forward with world-class research in data intensive social science. We think of data as an expansive category, one that is constantly changing as the research frontier moves. We offer a venue for methodological exchange from all corners of campus and across its bounds. \n",
      "\n",
      "D-Lab provides cross-disciplinary resources for in-depth consulting and advising access to staff support and training and provisioning for software and other infrastructure needs. Networking with other Berkeley centers and facilities and with our departments and schools, we offer our services to researchers across the disciplines and underwrite the breadth of excellence of Berkeleyâ€™s graduate programs and faculty research. D-Lab builds networks that Berkeley researchers can connect with users of social science data in the off-campus world.\n"
     ]
    }
   ],
   "source": [
    "print(example3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "45ce243b-60b8-4e28-a69b-858e9df3bd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Segmentation\n",
      "D-Lab helps Berkeley faculty, staff, and students move forward with world-class research in data intensive social science.\n",
      "We think of data as an expansive category, one that is constantly changing as the research frontier moves.\n",
      "We offer a venue for methodological exchange from all corners of campus and across its bounds. \n",
      "\n",
      "\n",
      "D-Lab provides cross-disciplinary resources for in-depth consulting and advising access to staff support and training and provisioning for software and other infrastructure needs.\n",
      "Networking with other Berkeley centers and facilities and with our departments and schools, we offer our services to researchers across the disciplines and underwrite the breadth of excellence of Berkeleyâ€™s graduate programs and faculty research.\n",
      "D-Lab builds networks that Berkeley researchers can connect with users of social science data in the off-campus world.\n",
      "\n",
      "Entity Detection:\n",
      "Berkeley GPE\n",
      "Berkeley ORG\n",
      "Berkeley ORG\n",
      "Berkeley ORG\n",
      "\n",
      "Noun Chunks:\n",
      "D-Lab\n",
      "Berkeley faculty\n",
      "staff\n",
      "students\n",
      "world-class research\n",
      "data intensive social science\n",
      "We\n",
      "data\n",
      "an expansive category\n",
      "that\n",
      "the research frontier moves\n",
      "We\n",
      "a venue\n",
      "methodological exchange\n",
      "all corners\n",
      "campus\n",
      "its bounds\n",
      "D-Lab\n",
      "cross-disciplinary resources\n",
      "in-depth consulting\n",
      "access\n",
      "staff support\n",
      "training\n",
      "provisioning\n",
      "software\n",
      "other infrastructure needs\n",
      "other Berkeley centers\n",
      "facilities\n",
      "our departments\n",
      "schools\n",
      "we\n",
      "our services\n",
      "researchers\n",
      "the disciplines\n",
      "the breadth\n",
      "excellence\n",
      "Berkeleyâ€™s graduate programs\n",
      "faculty research\n",
      "D-Lab\n",
      "networks\n",
      "that\n",
      "Berkeley researchers\n",
      "users\n",
      "social science data\n",
      "the off-campus world\n"
     ]
    }
   ],
   "source": [
    "# Sentence segmentation\n",
    "print('Sentence Segmentation')\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)\n",
    "\n",
    "# Entity detection\n",
    "print('\\nEntity Detection:')\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)\n",
    "\n",
    "# Noun chunks\n",
    "print('\\nNoun Chunks:')\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f379d0-edf6-4282-9075-cc168b7444a3",
   "metadata": {},
   "source": [
    "There's a whole lot else we can do with it! Check out `spaCy`'s documentation to see more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
