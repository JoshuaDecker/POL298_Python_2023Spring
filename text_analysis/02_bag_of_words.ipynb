{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ffcdc86-76ef-453f-a162-7741c9c23b8c",
   "metadata": {},
   "source": [
    "# Python Text Analysis: Fundamentals, Part 2\n",
    "\n",
    "In the previous lesson, we learned how to use several packages to preprocess text. However, we never moved beyond the *text representation* - we only manipulated the representation itself. If we want to perform computational analysis on the text, we will need to devise approaches to convert the text into a *numeric representation*.\n",
    "\n",
    "In this lesson, we'll explore one of the simplest ways to generate a numeric representation from text: the **bag-of-words**. With this numeric representation, we'll build a simple classifier and explore what the classifier tells us. At the heart of the bag-of-words approach lies the hypothesis that the presence and frequency of specific tokens is informative about the semantics and sentiment behind the text.\n",
    "\n",
    "We'll make heavy use of the `scikit-learn` package to do so, as it provides a nice framework for constructing the numeric representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4dbc3-52dd-4dc9-8d0d-21ad31808d09",
   "metadata": {},
   "source": [
    "## The Airline Tweets Dataset\n",
    "\n",
    "We'll work with a dataset consisting of tweets about US airlines. Each sample is a different tweet, which was posted at a specific airline. These tweets may express positive, neutral, or negative \"sentiment\". Our eventual goal will be to predict the sentiment of the tweet given its text.\n",
    "\n",
    "The dataset was collected by Crowdflower, which they then made public through Kaggle. We've already downloaded it and placed it in the `data` directory. Note that this dataset, on the whole, is structured nicely and has already undergone some cleaning. However, this is not the norm in real-life data science! We've chosen this dataset so that we can concentrate on learning and understanding the core concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcd13fe4-37b0-4a66-8cee-d45f61a8f3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (3.7)\n",
      "Requirement already satisfied: joblib in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: click in /Users/blueraspberry/anaconda3/lib/python3.10/site-packages (from nltk) (8.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e4a3a0d-66f4-44e5-8dd6-5f441146014d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/blueraspberry/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/blueraspberry/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66f537df-169a-4a08-bced-f9687238654a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4190e351-97b7-4c5b-866e-07aa6cbd42c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "# Use pandas to import tweets\n",
    "tweets_path = '../data/airline_tweets.csv'\n",
    "tweets = pd.read_csv(tweets_path, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880e8a36-bd58-4c24-8593-03a0ea70deed",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Before we ever do any preprocessing or modeling, we always should do some exploratory data analysis to get a feel for the dataset.\n",
    "\n",
    "First, let's take a look at the first few rows and all the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79acbaf2-6625-4abb-b50f-97ea54ba0d11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80232c78-ac41-4d74-a581-76c9dac3b8f6",
   "metadata": {},
   "source": [
    "We have a `tweet_id`, which uniquely identifies each tweet. We also have an `airline_sentiment`, which takes on values of `\"positive\"`, `\"negative\"`, or `\"neutral\"`. There are other columns indicating the author of the tweet, when it was created, the timezone of the user, and others. The main column of interest is the `text` column: these are the tweets. Let's take a look at a few:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "438830e6-1064-47fe-b578-a1ca693a0ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@VirginAmerica What @dhepburn said.\n",
      "@VirginAmerica plus you've added commercials to the experience... tacky.\n",
      "@VirginAmerica I didn't today... Must mean I need to take another trip!\n",
      "@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse\n",
      "@VirginAmerica and it's a really big bad thing about it\n"
     ]
    }
   ],
   "source": [
    "for idx in range(5):\n",
    "    print(tweets['text'].iloc[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6746f8-b29c-40d4-bef6-b4afd4cd6cc1",
   "metadata": {},
   "source": [
    "We can already see that some of these tweets very obviously negative sentiment - how can you tell this is the case? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0400a77f-7e79-44bf-81dc-40bf8953a6d1",
   "metadata": {},
   "source": [
    "Let's take a look at which airlines are tweeted about and how many of each in this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e98f355-830d-4d48-a1ad-16b1d8b29d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "airlines = tweets['airline']\n",
    "sns.countplot(x=airlines, order=airlines.value_counts().index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82c497c-4138-47eb-b959-235b552e7a47",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Challenge 1: Getting to Know the Data\n",
    "\n",
    "Use `pandas` to find out the following about the airline tweets:\n",
    "\n",
    "* How many tweets are in the dataset?\n",
    "* How many tweets are positive, neutral, and negative?\n",
    "* What *proportion* of tweets are positive, neutral, and negative?\n",
    "* Make a bar plot showing the proportion of tweet sentiments.\n",
    "\n",
    "If you have time, try the following:\n",
    "\n",
    "* How much time separates the earliest and latest tweets?\n",
    "* What gets more retweets: positive, negative, or neutral tweets?\n",
    "* Identify the airline whose tweets have the highest proportion of negative sentiment.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b025a6b7-d7ec-46e1-99f5-172ee01c7693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14640"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many tweets are in the dataset?\n",
    "tweets.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "076fade4-eb6f-46ca-bba3-f045b542800b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    9178\n",
       "neutral     3099\n",
       "positive    2363\n",
       "Name: airline_sentiment, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many tweets are positive, neutral, and negative?\n",
    "tweets['airline_sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "411531cd-d174-4670-8775-7740f8ec94b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    0.626913\n",
       "neutral     0.211680\n",
       "positive    0.161407\n",
       "Name: airline_sentiment, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What *proportion* of tweets are positive, neutral, and negative?\n",
    "tweets['airline_sentiment'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76f37bf0-7d25-4997-a59a-58bdbabe9697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='airline_sentiment', ylabel='count'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkQAAAGxCAYAAACDV6ltAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAwhElEQVR4nO3de1RV5b7/8c8SFFFhqSggRV4STdPymoEVHG+Uecn21hKi3JqXVAjNrTnKtE7J0b29lJ5M3aVmlu2xy7KToWjJ8a6heM88hpdOEJq4ECVQmL8/Os5fS9QUkYU+79cYa4w9n/mdc37nGnPLp2fOtZbDsixLAAAABqvk6QYAAAA8jUAEAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADCet6cbuFkUFxfrp59+kp+fnxwOh6fbAQAAV8GyLJ0+fVohISGqVOny80AEoqv0008/KTQ01NNtAACAUjh27Jhuv/32y64nEF0lPz8/Sb+9of7+/h7uBgAAXI3c3FyFhobaf8cvh0B0lS7cJvP39ycQAQBwk/mjx114qBoAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPG9PNwAAwO91nNXR0y2gAtkQv6FcjsMMEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4Hg1E58+f18svv6yGDRvK19dXjRo10muvvabi4mK7xrIsTZo0SSEhIfL19VVUVJT27t3rtp+CggLFx8erTp06ql69unr16qUff/zRrSYnJ0dxcXFyOp1yOp2Ki4vTqVOnyuM0AQBABefRQDRlyhS98847mj17tvbv36+pU6fqb3/7m2bNmmXXTJ06VdOnT9fs2bO1bds2BQcHq2vXrjp9+rRdk5iYqGXLlmnp0qVav3698vLy1KNHDxUVFdk1MTExSk9PV3JyspKTk5Wenq64uLhyPV8AAFAxOSzLsjx18B49eigoKEjvvvuuPfanP/1J1apV0+LFi2VZlkJCQpSYmKhx48ZJ+m02KCgoSFOmTNHQoUPlcrlUt25dLV68WE888YQk6aefflJoaKhWrFih6Oho7d+/X82bN9fmzZvVoUMHSdLmzZsVHh6u7777Tk2bNv3DXnNzc+V0OuVyueTv738D3g0AgCR1nNXR0y2gAtkQv+G6tr/av98enSF64IEHtGbNGn3//feSpJ07d2r9+vXq3r27JCkjI0NZWVnq1q2bvY2Pj48iIyO1ceNGSVJaWprOnTvnVhMSEqIWLVrYNZs2bZLT6bTDkCTdf//9cjqddg0AADCXtycPPm7cOLlcLt11113y8vJSUVGR3njjDfXv31+SlJWVJUkKCgpy2y4oKEhHjhyxa6pUqaJatWqVqLmwfVZWlgIDA0scPzAw0K65WEFBgQoKCuzl3NzcUp4lAACo6Dw6Q/Txxx/rgw8+0Icffqjt27dr0aJF+vvf/65Fixa51TkcDrdly7JKjF3s4ppL1V9pP0lJSfYD2E6nU6GhoVd7WgAA4Cbj0UD017/+VS+++KKefPJJtWzZUnFxcRo1apSSkpIkScHBwZJUYhYnOzvbnjUKDg5WYWGhcnJyrljz888/lzj+8ePHS8w+XTB+/Hi5XC77dezYses7WQAAUGF5NBCdPXtWlSq5t+Dl5WV/7L5hw4YKDg5WSkqKvb6wsFCpqamKiIiQJLVt21aVK1d2q8nMzNSePXvsmvDwcLlcLm3dutWu2bJli1wul11zMR8fH/n7+7u9AADArcmjzxD17NlTb7zxhu644w7dfffd2rFjh6ZPn66BAwdK+u02V2JioiZPnqywsDCFhYVp8uTJqlatmmJiYiRJTqdTgwYN0gsvvKCAgADVrl1bY8aMUcuWLdWlSxdJUrNmzfTwww9r8ODBmjt3riRpyJAh6tGjx1V9wgwAANzaPBqIZs2apQkTJmj48OHKzs5WSEiIhg4dqldeecWuGTt2rPLz8zV8+HDl5OSoQ4cOWrVqlfz8/OyaGTNmyNvbW/369VN+fr46d+6shQsXysvLy65ZsmSJEhIS7E+j9erVS7Nnzy6/kwUAABWWR7+H6GbC9xABQPnge4jwe0Z8DxEAAEBFQCACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxvN4IPrf//1fPfXUUwoICFC1atXUqlUrpaWl2esty9KkSZMUEhIiX19fRUVFae/evW77KCgoUHx8vOrUqaPq1aurV69e+vHHH91qcnJyFBcXJ6fTKafTqbi4OJ06dao8ThEAAFRwHg1EOTk56tixoypXrqyvvvpK+/bt07Rp01SzZk27ZurUqZo+fbpmz56tbdu2KTg4WF27dtXp06ftmsTERC1btkxLly7V+vXrlZeXpx49eqioqMiuiYmJUXp6upKTk5WcnKz09HTFxcWV5+kCAIAKymFZluWpg7/44ovasGGD1q1bd8n1lmUpJCREiYmJGjdunKTfZoOCgoI0ZcoUDR06VC6XS3Xr1tXixYv1xBNPSJJ++uknhYaGasWKFYqOjtb+/fvVvHlzbd68WR06dJAkbd68WeHh4fruu+/UtGnTP+w1NzdXTqdTLpdL/v7+ZfQOAAAu1nFWR0+3gApkQ/yG69r+av9+e3SGaPny5WrXrp369u2rwMBAtW7dWvPnz7fXZ2RkKCsrS926dbPHfHx8FBkZqY0bN0qS0tLSdO7cObeakJAQtWjRwq7ZtGmTnE6nHYYk6f7775fT6bRrLlZQUKDc3Fy3FwAAuDV5NBD98MMPmjNnjsLCwrRy5UoNGzZMCQkJev/99yVJWVlZkqSgoCC37YKCgux1WVlZqlKlimrVqnXFmsDAwBLHDwwMtGsulpSUZD9v5HQ6FRoaen0nCwAAKiyPBqLi4mK1adNGkydPVuvWrTV06FANHjxYc+bMcatzOBxuy5ZllRi72MU1l6q/0n7Gjx8vl8tlv44dO3a1pwUAAG4yHg1E9erVU/Pmzd3GmjVrpqNHj0qSgoODJanELE52drY9axQcHKzCwkLl5ORcsebnn38ucfzjx4+XmH26wMfHR/7+/m4vAABwa/JoIOrYsaMOHDjgNvb999+rfv36kqSGDRsqODhYKSkp9vrCwkKlpqYqIiJCktS2bVtVrlzZrSYzM1N79uyxa8LDw+VyubR161a7ZsuWLXK5XHYNAAAwl7cnDz5q1ChFRERo8uTJ6tevn7Zu3ap58+Zp3rx5kn67zZWYmKjJkycrLCxMYWFhmjx5sqpVq6aYmBhJktPp1KBBg/TCCy8oICBAtWvX1pgxY9SyZUt16dJF0m+zTg8//LAGDx6suXPnSpKGDBmiHj16XNUnzAAAwK3No4Goffv2WrZsmcaPH6/XXntNDRs21MyZMxUbG2vXjB07Vvn5+Ro+fLhycnLUoUMHrVq1Sn5+fnbNjBkz5O3trX79+ik/P1+dO3fWwoUL5eXlZdcsWbJECQkJ9qfRevXqpdmzZ5ffyQIAgArLo99DdDPhe4gAoHzwPUT4PSO+hwgAAKAiIBABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgvFIFok6dOunUqVMlxnNzc9WpU6fr7QkAAKBclSoQrV27VoWFhSXGf/31V61bt+66mwIAAChP3tdSvGvXLvt/79u3T1lZWfZyUVGRkpOTddttt5VddwAAAOXgmgJRq1at5HA45HA4LnlrzNfXV7NmzSqz5gAAAMrDNQWijIwMWZalRo0aaevWrapbt669rkqVKgoMDJSXl1eZNwkAAHAjXVMgql+/viSpuLj4hjQDAADgCdcUiH7v+++/19q1a5WdnV0iIL3yyivX3RgAAEB5KVUgmj9/vp577jnVqVNHwcHBcjgc9jqHw0EgAgAAN5VSBaLXX39db7zxhsaNG1fW/QAAAJS7Un0PUU5Ojvr27VvWvQAAAHhEqQJR3759tWrVqrLuBQAAwCNKdcuscePGmjBhgjZv3qyWLVuqcuXKbusTEhLKpDkAAIDy4LAsy7rWjRo2bHj5HToc+uGHH66rqYooNzdXTqdTLpdL/v7+nm4HAG5ZHWd19HQLqEA2xG+4ru2v9u93qWaIMjIySt0YAABARVOqZ4gAAABuJaWaIRo4cOAV17/33nulagYAAMATShWIcnJy3JbPnTunPXv26NSpU5f80VcAAICKrFSBaNmyZSXGiouLNXz4cDVq1Oi6mwIAAChPZfYMUaVKlTRq1CjNmDGjrHYJAABQLsr0oepDhw7p/PnzZblLAACAG65Ut8xGjx7ttmxZljIzM/Xll1/qmWeeKZPGAAAAykupAtGOHTvclitVqqS6detq2rRpf/gJNAAAgIqmVIHom2++Kes+AAAAPKZUgeiC48eP68CBA3I4HGrSpInq1q1bVn0BAACUm1I9VH3mzBkNHDhQ9erV00MPPaQHH3xQISEhGjRokM6ePVvWPQIAANxQpQpEo0ePVmpqqr744gudOnVKp06d0ueff67U1FS98MILZd0jAADADVWqW2affPKJ/vWvfykqKsoe6969u3x9fdWvXz/NmTOnrPoDAAC44Uo1Q3T27FkFBQWVGA8MDOSWGQAAuOmUKhCFh4dr4sSJ+vXXX+2x/Px8vfrqqwoPDy+z5gAAAMpDqW6ZzZw5U4888ohuv/123XvvvXI4HEpPT5ePj49WrVpV1j0CAADcUKUKRC1bttTBgwf1wQcf6LvvvpNlWXryyScVGxsrX1/fsu4RAADghipVIEpKSlJQUJAGDx7sNv7ee+/p+PHjGjduXJk0BwAAUB5K9QzR3Llzddddd5UYv/vuu/XOO+9cd1MAAADlqVSBKCsrS/Xq1SsxXrduXWVmZl53UwAAAOWpVIEoNDRUGzZsKDG+YcMGhYSEXHdTAAAA5alUzxA9++yzSkxM1Llz59SpUydJ0po1azR27Fi+qRoAANx0ShWIxo4dq5MnT2r48OEqLCyUJFWtWlXjxo3T+PHjy7RBAACAG61UgcjhcGjKlCmaMGGC9u/fL19fX4WFhcnHx6es+wMAALjhShWILqhRo4bat29fVr0AAAB4RKkeqgYAALiVEIgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABivwgSipKQkORwOJSYm2mOWZWnSpEkKCQmRr6+voqKitHfvXrftCgoKFB8frzp16qh69erq1auXfvzxR7eanJwcxcXFyel0yul0Ki4uTqdOnSqHswIAADeDChGItm3bpnnz5umee+5xG586daqmT5+u2bNna9u2bQoODlbXrl11+vRpuyYxMVHLli3T0qVLtX79euXl5alHjx4qKiqya2JiYpSenq7k5GQlJycrPT1dcXFx5XZ+AACgYvN4IMrLy1NsbKzmz5+vWrVq2eOWZWnmzJl66aWX9Pjjj6tFixZatGiRzp49qw8//FCS5HK59O6772ratGnq0qWLWrdurQ8++EC7d+/W6tWrJUn79+9XcnKy/vGPfyg8PFzh4eGaP3++/uu//ksHDhzwyDkDAICKxeOBaMSIEXr00UfVpUsXt/GMjAxlZWWpW7du9piPj48iIyO1ceNGSVJaWprOnTvnVhMSEqIWLVrYNZs2bZLT6VSHDh3smvvvv19Op9OuuZSCggLl5ua6vQAAwK3pun6643otXbpU27dv17Zt20qsy8rKkiQFBQW5jQcFBenIkSN2TZUqVdxmli7UXNg+KytLgYGBJfYfGBho11xKUlKSXn311Ws7IQAAcFPy2AzRsWPH9Pzzz+uDDz5Q1apVL1vncDjcli3LKjF2sYtrLlX/R/sZP368XC6X/Tp27NgVjwkAAG5eHgtEaWlpys7OVtu2beXt7S1vb2+lpqbqrbfekre3tz0zdPEsTnZ2tr0uODhYhYWFysnJuWLNzz//XOL4x48fLzH79Hs+Pj7y9/d3ewEAgFuTxwJR586dtXv3bqWnp9uvdu3aKTY2Vunp6WrUqJGCg4OVkpJib1NYWKjU1FRFRERIktq2bavKlSu71WRmZmrPnj12TXh4uFwul7Zu3WrXbNmyRS6Xy64BAABm89gzRH5+fmrRooXbWPXq1RUQEGCPJyYmavLkyQoLC1NYWJgmT56satWqKSYmRpLkdDo1aNAgvfDCCwoICFDt2rU1ZswYtWzZ0n5Iu1mzZnr44Yc1ePBgzZ07V5I0ZMgQ9ejRQ02bNi3HMwYAABWVRx+q/iNjx45Vfn6+hg8frpycHHXo0EGrVq2Sn5+fXTNjxgx5e3urX79+ys/PV+fOnbVw4UJ5eXnZNUuWLFFCQoL9abRevXpp9uzZ5X4+AACgYnJYlmV5uombQW5urpxOp1wuF88TAcAN1HFWR0+3gApkQ/yG69r+av9+e/x7iAAAADyNQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABiPQAQAAIxHIAIAAMYjEAEAAON5e7oBAJ519LWWnm4BFcgdr+z2dAuARzBDBAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA43l7ugGTtP3r+55uARVM2t+e9nQLAAAxQwQAAEAgAgAAIBABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABjPo4EoKSlJ7du3l5+fnwIDA/XYY4/pwIEDbjWWZWnSpEkKCQmRr6+voqKitHfvXreagoICxcfHq06dOqpevbp69eqlH3/80a0mJydHcXFxcjqdcjqdiouL06lTp270KQIAgJuARwNRamqqRowYoc2bNyslJUXnz59Xt27ddObMGbtm6tSpmj59umbPnq1t27YpODhYXbt21enTp+2axMRELVu2TEuXLtX69euVl5enHj16qKioyK6JiYlRenq6kpOTlZycrPT0dMXFxZXr+QIAgIrJoz/dkZyc7La8YMECBQYGKi0tTQ899JAsy9LMmTP10ksv6fHHH5ckLVq0SEFBQfrwww81dOhQuVwuvfvuu1q8eLG6dOkiSfrggw8UGhqq1atXKzo6Wvv371dycrI2b96sDh06SJLmz5+v8PBwHThwQE2bNi3fEwcAABVKhXqGyOVySZJq164tScrIyFBWVpa6detm1/j4+CgyMlIbN26UJKWlpencuXNuNSEhIWrRooVds2nTJjmdTjsMSdL9998vp9Np1wAAAHNVmB93tSxLo0eP1gMPPKAWLVpIkrKysiRJQUFBbrVBQUE6cuSIXVOlShXVqlWrRM2F7bOyshQYGFjimIGBgXbNxQoKClRQUGAv5+bmlvLMAABARVdhZohGjhypXbt26aOPPiqxzuFwuC1bllVi7GIX11yq/kr7SUpKsh/AdjqdCg0NvZrTAAAAN6EKEYji4+O1fPlyffPNN7r99tvt8eDgYEkqMYuTnZ1tzxoFBwersLBQOTk5V6z5+eefSxz3+PHjJWafLhg/frxcLpf9OnbsWOlPEAAAVGgeDUSWZWnkyJH69NNP9fXXX6thw4Zu6xs2bKjg4GClpKTYY4WFhUpNTVVERIQkqW3btqpcubJbTWZmpvbs2WPXhIeHy+VyaevWrXbNli1b5HK57JqL+fj4yN/f3+0FAABuTR59hmjEiBH68MMP9fnnn8vPz8+eCXI6nfL19ZXD4VBiYqImT56ssLAwhYWFafLkyapWrZpiYmLs2kGDBumFF15QQECAateurTFjxqhly5b2p86aNWumhx9+WIMHD9bcuXMlSUOGDFGPHj34hBkAAPBsIJozZ44kKSoqym18wYIFGjBggCRp7Nixys/P1/Dhw5WTk6MOHTpo1apV8vPzs+tnzJghb29v9evXT/n5+ercubMWLlwoLy8vu2bJkiVKSEiwP43Wq1cvzZ49+8aeIAAAuCk4LMuyPN3EzSA3N1dOp1Mul6vUt8/a/vX9Mu4KN7u0vz3t6RZ09LWWnm4BFcgdr+z2dAvqOKujp1tABbIhfsN1bX+1f78rxEPVAAAAnkQgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABiPQAQAAIxHIAIAAMYzKhC9/fbbatiwoapWraq2bdtq3bp1nm4JAABUAMYEoo8//liJiYl66aWXtGPHDj344IN65JFHdPToUU+3BgAAPMyYQDR9+nQNGjRIzz77rJo1a6aZM2cqNDRUc+bM8XRrAADAw4wIRIWFhUpLS1O3bt3cxrt166aNGzd6qCsAAFBReHu6gfJw4sQJFRUVKSgoyG08KChIWVlZl9ymoKBABQUF9rLL5ZIk5ebmlrqPooL8Um+LW9P1XE9l5fSvRZ5uARVIRbgmz+ef93QLqECu95q8sL1lWVesMyIQXeBwONyWLcsqMXZBUlKSXn311RLjoaGhN6Q3mMk5a5inWwDcJTk93QHgxjmubK7J06dPy+m8/L6MCER16tSRl5dXidmg7OzsErNGF4wfP16jR4+2l4uLi3Xy5EkFBARcNkThj+Xm5io0NFTHjh2Tv7+/p9sBJHFdouLhmiw7lmXp9OnTCgkJuWKdEYGoSpUqatu2rVJSUtSnTx97PCUlRb17977kNj4+PvLx8XEbq1mz5o1s0yj+/v78nxwVDtclKhquybJxpZmhC4wIRJI0evRoxcXFqV27dgoPD9e8efN09OhRDRvGLQsAAExnTCB64okn9Msvv+i1115TZmamWrRooRUrVqh+/fqebg0AAHiYMYFIkoYPH67hw4d7ug2j+fj4aOLEiSVuRwKexHWJioZrsvw5rD/6HBoAAMAtzogvZgQAALgSAhEAADAegQjlYu3atXI4HDp16tQV6xo0aKCZM2eWS0/Ajcb1jPIwadIktWrVytNt3PQIRCgXERERyszMtL8LYuHChZf8Xqdt27ZpyJAh5dwd8JuoqCglJiZ6ug3gshwOhz777DO3sTFjxmjNmjWeaegWYtSnzOA5VapUUXBw8B/W1a1btxy6AUrPsiwVFRXJ25t/PlEx1KhRQzVq1PB0Gzc9Zohgi4qK0siRIzVy5EjVrFlTAQEBevnll+0fxMvJydHTTz+tWrVqqVq1anrkkUd08OBBe/sjR46oZ8+eqlWrlqpXr667775bK1askOR+y2zt2rX6y1/+IpfLJYfDIYfDoUmTJklyv8XQv39/Pfnkk249njt3TnXq1NGCBQsk/fbHaerUqWrUqJF8fX1177336l//+tcNfqfgCVFRUUpISNDYsWNVu3ZtBQcH29eN9NsPMA8ZMkSBgYHy9/dXp06dtHPnTnv9gAED9Nhjj7ntMzExUVFRUfb61NRUvfnmm/Z1efjwYfvaXblypdq1aycfHx+tW7dOhw4dUu/evRUUFKQaNWqoffv2Wr16dTm8E/CE673+JOn1119XYGCg/Pz89Oyzz+rFF190u9W1bds2de3aVXXq1JHT6VRkZKS2b99ur2/QoIEkqU+fPnI4HPby72+ZrVy5UlWrVi3xeEJCQoIiIyPt5Y0bN+qhhx6Sr6+vQkNDlZCQoDNnzlz3+3QzIxDBzaJFi+Tt7a0tW7borbfe0owZM/SPf/xD0m9/ML799lstX75cmzZtkmVZ6t69u86dOydJGjFihAoKCvTf//3f2r17t6ZMmXLJ/2qJiIjQzJkz5e/vr8zMTGVmZmrMmDEl6mJjY7V8+XLl5eXZYytXrtSZM2f0pz/9SZL08ssva8GCBZozZ4727t2rUaNG6amnnlJqauqNeHvgYYsWLVL16tW1ZcsWTZ06Va+99ppSUlJkWZYeffRRZWVlacWKFUpLS1ObNm3UuXNnnTx58qr2/eabbyo8PFyDBw+2r8vf/5jz2LFjlZSUpP379+uee+5RXl6eunfvrtWrV2vHjh2Kjo5Wz549dfTo0Rt1+vCw67n+lixZojfeeENTpkxRWlqa7rjjDs2ZM8dt/6dPn9YzzzyjdevWafPmzQoLC1P37t11+vRpSb8FJklasGCBMjMz7eXf69Kli2rWrKlPPvnEHisqKtI///lPxcbGSpJ2796t6OhoPf7449q1a5c+/vhjrV+/XiNHjrwh79tNwwL+T2RkpNWsWTOruLjYHhs3bpzVrFkz6/vvv7ckWRs2bLDXnThxwvL19bX++c9/WpZlWS1btrQmTZp0yX1/8803liQrJyfHsizLWrBggeV0OkvU1a9f35oxY4ZlWZZVWFho1alTx3r//fft9f3797f69u1rWZZl5eXlWVWrVrU2btzoto9BgwZZ/fv3v+bzR8UWGRlpPfDAA25j7du3t8aNG2etWbPG8vf3t3799Ve39Xfeeac1d+5cy7Is65lnnrF69+7ttv7555+3IiMj3Y7x/PPPu9VcuHY/++yzP+yxefPm1qxZs+zl31/PuLld7/XXoUMHa8SIEW7rO3bsaN17772XPeb58+ctPz8/64svvrDHJFnLli1zq5s4caLbfhISEqxOnTrZyytXrrSqVKlinTx50rIsy4qLi7OGDBnito9169ZZlSpVsvLz8y/bz62OGSK4uf/+++VwOOzl8PBwHTx4UPv27ZO3t7c6dOhgrwsICFDTpk21f/9+Sb9Nyb7++uvq2LGjJk6cqF27dl1XL5UrV1bfvn21ZMkSSdKZM2f0+eef2/+Vs2/fPv3666/q2rWrfQ+9Ro0aev/993Xo0KHrOjYqpnvuucdtuV69esrOzlZaWpry8vIUEBDgdi1kZGSU2bXQrl07t+UzZ85o7Nixat68uWrWrKkaNWrou+++Y4boFnY919+BAwd03333uW1/8XJ2draGDRumJk2ayOl0yul0Ki8v75qvqdjYWK1du1Y//fSTpN9mp7p3765atWpJktLS0rRw4UK3XqOjo1VcXKyMjIxrOtathKcCcV0sy7ID1LPPPqvo6Gh9+eWXWrVqlZKSkjRt2jTFx8eXev+xsbGKjIxUdna2UlJSVLVqVT3yyCOSpOLiYknSl19+qdtuu81tO77u/tZUuXJlt2WHw6Hi4mIVFxerXr16Wrt2bYltLnyasVKlSvbzcBdcuN17NapXr+62/Ne//lUrV67U3//+dzVu3Fi+vr7685//rMLCwqveJ24u13P9Xaj/vYuvxwEDBuj48eOaOXOm6tevLx8fH4WHh1/zNXXffffpzjvv1NKlS/Xcc89p2bJl9nOX0m//dg4dOlQJCQkltr3jjjuu6Vi3EgIR3GzevLnEclhYmJo3b67z589ry5YtioiIkCT98ssv+v7779WsWTO7PjQ0VMOGDdOwYcM0fvx4zZ8//5KBqEqVKioqKvrDfiIiIhQaGqqPP/5YX331lfr27asqVapIkpo3by4fHx8dPXrU7WFBmKdNmzbKysqSt7e3/aDpxerWras9e/a4jaWnp7v9kbva61KS1q1bpwEDBqhPnz6SpLy8PB0+fLhU/ePmdjXXX9OmTbV161bFxcXZY99++61bzbp16/T222+re/fukqRjx47pxIkTbjWVK1e+qms0JiZGS5Ys0e23365KlSrp0Ucfdet37969aty48dWeohG4ZQY3x44d0+jRo3XgwAF99NFHmjVrlp5//nmFhYWpd+/eGjx4sNavX6+dO3fqqaee0m233abevXtL+u0TOytXrlRGRoa2b9+ur7/+2i0s/V6DBg2Ul5enNWvW6MSJEzp79uwl6xwOh2JiYvTOO+8oJSVFTz31lL3Oz89PY8aM0ahRo7Ro0SIdOnRIO3bs0H/+539q0aJFZf/moMLq0qWLwsPD9dhjj2nlypU6fPiwNm7cqJdfftn+o9OpUyd9++23ev/993Xw4EFNnDixREBq0KCBtmzZosOHD+vEiRP2LOSlNG7cWJ9++qnS09O1c+dOxcTEXLEet66ruf7i4+P17rvvatGiRTp48KBef/117dq1y23WqHHjxlq8eLH279+vLVu2KDY2Vr6+vm7HatCggdasWaOsrCzl5ORctqfY2Fht375db7zxhv785z+ratWq9rpx48Zp06ZNGjFihNLT03Xw4EEtX778umbzbwUEIrh5+umnlZ+fr/vuu08jRoxQfHy8/UWJCxYsUNu2bdWjRw+Fh4fLsiytWLHC/i/soqIijRgxQs2aNdPDDz+spk2b6u23377kcSIiIjRs2DA98cQTqlu3rqZOnXrZnmJjY7Vv3z7ddttt6tixo9u6f//3f9crr7yipKQkNWvWTNHR0friiy/UsGHDMnpHcDNwOBxasWKFHnroIQ0cOFBNmjTRk08+qcOHDysoKEiSFB0drQkTJmjs2LFq3769Tp8+raefftptP2PGjJGXl5eaN2+uunXrXvHZjRkzZqhWrVqKiIhQz549FR0drTZt2tzQ80TFdDXXX2xsrMaPH68xY8aoTZs2ysjI0IABA9yCynvvvaecnBy1bt1acXFxSkhIUGBgoNuxpk2bppSUFIWGhqp169aX7SksLEzt27fXrl277OcuL7jnnnuUmpqqgwcP6sEHH1Tr1q01YcIE1atXrwzflZsPv3YPW1RUlFq1asVPDQBAOejatauCg4O1ePFiT7cC8QwRAAA33NmzZ/XOO+8oOjpaXl5e+uijj7R69WqlpKR4ujX8HwIRAAA32IXbaq+//roKCgrUtGlTffLJJ+rSpYunW8P/4ZYZAAAwHg9VAwAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIQKkdPnxYDodD6enpV6ybNGmSWrVqZS8PGDBAjz322A3trSIw5TyBWwHfQwSg1EJDQ5WZmak6depc03ZvvvlmiV/6vpkdPnxYDRs21I4dO9yCX0U6T4fDoWXLlhHQgMsgEAEoNS8vLwUHB192vWVZl/xlbqfTeSPbqjBMOU/gVsAtMwBXlJycrAceeEA1a9ZUQECAevTooUOHDkkqects7dq1cjgcWrlypdq1aycfHx+tW7euxD4vvpUUFRWlhIQEjR07VrVr11ZwcLAmTZrkto3L5dKQIUMUGBgof39/derUSTt37ryqc9i5c6f+7d/+TX5+fvL391fbtm3tXyGXpI0bN+qhhx6Sr6+vQkNDlZCQoDNnztjrGzRooMmTJ2vgwIHy8/PTHXfcoXnz5tnrL/yYcOvWreVwOBQVFXXZ84yPj1diYqJq1aqloKAgzZs3T2fOnNFf/vIX+fn56c4779RXX33l1v++ffvUvXt31ahRQ0FBQYqLi9OJEyeu+v1r0KCBJKlPnz5yOBz2MoD/j0AE4IrOnDmj0aNHa9u2bVqzZo0qVaqkPn36qLi4+LLbjB07VklJSdq/f7/uueeeqzrOokWLVL16dW3ZskVTp07Va6+9Zv/Ok2VZevTRR5WVlaUVK1YoLS1Nbdq0UefOnXXy5Mk/3HdsbKxuv/12bdu2TWlpaXrxxRdVuXJlSdLu3bsVHR2txx9/XLt27dLHH3+s9evXa+TIkW77mDZtmtq1a6cdO3Zo+PDheu655/Tdd99JkrZu3SpJWr16tTIzM/Xpp59e8Tzr1KmjrVu3Kj4+Xs8995z69u2riIgIbd++XdHR0YqLi9PZs2clSZmZmYqMjFSrVq307bffKjk5WT///LP69et31e/ftm3bJEkLFixQZmamvQzgdywAuAbZ2dmWJGv37t1WRkaGJcnasWOHZVmW9c0331iSrM8++8xtm4kTJ1r33nuvvfzMM89YvXv3tpcjIyOtBx54wG2b9u3bW+PGjbMsy7LWrFlj+fv7W7/++qtbzZ133mnNnTv3D3v28/OzFi5ceMl1cXFx1pAhQ9zG1q1bZ1WqVMnKz8+3LMuy6tevbz311FP2+uLiYiswMNCaM2eOZVlWiffhas/z/PnzVvXq1a24uDh7LDMz05Jkbdq0ybIsy5owYYLVrVs3t/0eO3bMkmQdOHDgkvu1LPf3z7IsS5K1bNmyS74HACyLGSIAV3To0CHFxMSoUaNG8vf3t28PHT169LLbtGvX7pqPc/FMUr169ZSdnS1JSktLU15engICAlSjRg37lZGRYd++u5LRo0fr2WefVZcuXfQf//EfbtukpaVp4cKFbvuNjo5WcXGxMjIyLtmfw+FQcHCw3V9pz9PLy0sBAQFq2bKlPRYUFCRJbuf+zTffuPV31113SZLbeVzp/QPwx3ioGsAV9ezZU6GhoZo/f75CQkJUXFysFi1aqLCw8LLbVK9e/ZqPc+EW1gUOh8O+LVdcXKx69epp7dq1JbarWbPmH+570qRJiomJ0ZdffqmvvvpKEydO1NKlS+1bf0OHDlVCQkKJ7e64446r6u9aXGo/vx9zOByS5HbuPXv21JQpU0rsq169emXeH2AqAhGAy/rll1+0f/9+zZ07Vw8++KAkaf369eXeR5s2bZSVlSVvb+9SPxDcpEkTNWnSRKNGjVL//v21YMEC9enTR23atNHevXvVuHHjUvdXpUoVSbrkJ+quV5s2bfTJJ5+oQYMG8vYu/T/ZlStXviH9AbcKbpkBuKxatWopICBA8+bN0//8z//o66+/1ujRo8u9jy5duig8PFyPPfaYVq5cqcOHD2vjxo16+eWX3T4tdin5+fkaOXKk1q5dqyNHjmjDhg3atm2bmjVrJkkaN26cNm3apBEjRig9PV0HDx7U8uXLFR8ff9X9BQYGytfX137g2eVyXdf5/t6IESN08uRJ9e/fX1u3btUPP/ygVatWaeDAgdcUcBo0aKA1a9YoKytLOTk5ZdYfcKsgEAG4rEqVKmnp0qVKS0tTixYtNGrUKP3tb38r9z4cDodWrFihhx56SAMHDlSTJk305JNP6vDhw/YzN5fj5eWlX375RU8//bSaNGmifv366ZFHHtGrr74q6bdnb1JTU3Xw4EE9+OCDat26tSZMmOB2O+qPeHt766233tLcuXMVEhKi3r17X9f5/l5ISIg2bNigoqIiRUdHq0WLFnr++efldDpVqdLV/xM+bdo0paSkKDQ0VK1bty6z/oBbhcOyKsjXqAIAAHgIM0QAAMB4BCIAN727777b7WPpv38tWbLE0+0BuAlwywzATe/IkSM6d+7cJdcFBQXJz8+vnDsCcLMhEAEAAONxywwAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMN7/A7VZ/aA5hPuTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make a bar plot showing the proportion of tweet sentiments\n",
    "sns.countplot(x=tweets['airline_sentiment'], order=['positive', 'neutral', 'negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fe70919-8d77-485a-8d9a-9ec0b805a2e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timedelta('7 days 12:17:32')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How much time separates the earliest and latest tweets?\n",
    "sorted_by_time = pd.to_datetime(tweets['tweet_created'].sort_values())\n",
    "sorted_by_time.iloc[-1] - sorted_by_time.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcb0acb5-b5c3-43ab-b3b8-967e6e398943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airline_sentiment\n",
       "negative    0.093375\n",
       "neutral     0.060987\n",
       "positive    0.069403\n",
       "Name: retweet_count, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What gets more retweets: positive, negative, or neutral tweets?\n",
    "tweets.groupby('airline_sentiment')['retweet_count'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f4cc562-1587-4f7b-91e9-70e3b5cd393f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>airline</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Virgin America</th>\n",
       "      <td>0.359127</td>\n",
       "      <td>0.339286</td>\n",
       "      <td>0.301587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Delta</th>\n",
       "      <td>0.429793</td>\n",
       "      <td>0.325383</td>\n",
       "      <td>0.244824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Southwest</th>\n",
       "      <td>0.490083</td>\n",
       "      <td>0.274380</td>\n",
       "      <td>0.235537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United</th>\n",
       "      <td>0.688906</td>\n",
       "      <td>0.182365</td>\n",
       "      <td>0.128728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>American</th>\n",
       "      <td>0.710402</td>\n",
       "      <td>0.167814</td>\n",
       "      <td>0.121783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US Airways</th>\n",
       "      <td>0.776862</td>\n",
       "      <td>0.130793</td>\n",
       "      <td>0.092345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "airline_sentiment  negative   neutral  positive\n",
       "airline                                        \n",
       "Virgin America     0.359127  0.339286  0.301587\n",
       "Delta              0.429793  0.325383  0.244824\n",
       "Southwest          0.490083  0.274380  0.235537\n",
       "United             0.688906  0.182365  0.128728\n",
       "American           0.710402  0.167814  0.121783\n",
       "US Airways         0.776862  0.130793  0.092345"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which airline receives the highest proportion of negative tweets?\n",
    "proportions = tweets.groupby(['airline', 'airline_sentiment']).size() / tweets.groupby('airline').size()\n",
    "proportions.unstack().sort_values('negative')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e513930-2dc7-489c-bc5a-22eb09add5bf",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "We spent much of the last workshop learning how to preprocess the data. Let's apply what we learned to this dataset. Looking at some of the tweets above, we can see that while they are in pretty good shape, we can do some additional processing on them.\n",
    "\n",
    "In our pipeline, we'll omit the tokenization process, since we will perform it in a later step. Instead, we'll add a couple new preprocessing steps now that we are working with social media data. Specifically, we'll replace all hashtags with a \"HASHTAG\" token, and we'll replace all users (denoted by the \"@\" symbol) with a \"USER\" token. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a83ece-f3b2-4200-9d22-0788fbc07fa4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Challenge 2: Creating a Preprocessing Pipeline for Social Media Data\n",
    "\n",
    "Write a function called `preprocess()` that performs the following on a text input:\n",
    "\n",
    "* Lowercase text.\n",
    "* Replace all URLs with the token \"URL\".\n",
    "* Replace all numbers with the token \"DIGIT\".\n",
    "* Replace hashtags with the token \"HASHTAG\".\n",
    "* Replace all users with the token \"USER\".\n",
    "* Remove blankspaces.\n",
    "\n",
    "We have provided regex patterns for each of the replacement steps in the following cells.\n",
    "\n",
    "Run your `preprocess()` function on `example_tweet` (two cells below), and when you think you have it working, apply it to the entire `text` column in the tweets DataFrame.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb499df1-b1ce-41f8-a41b-3a3fbf51bbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful regex patterns\n",
    "url_pattern = r'(http|ftp|https):\\/\\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\\/~+#-]*[\\w@?^=%&\\/~+#-])'\n",
    "digit_pattern = '\\d+'\n",
    "hashtag_pattern = r'(?:^|\\s)[＃#]{1}(\\w+)'\n",
    "user_pattern = r'@(\\w+)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75bb1fab-3ad6-486d-81f1-b294cf5e653a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply your function to the following example\n",
    "example_tweet = \"lol @justinbeiber and @BillGates are like soo 2000 #yesterday #amiright saw it on https://twitter.com #yolo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33052a00-679c-4f78-b934-1634c144d202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    # YOUR CODE HERE\n",
    "    \"\"\"Preprocesses a string.\"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Replace URLs\n",
    "    url_pattern = r'(http|ftp|https):\\/\\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\\/~+#-]*[\\w@?^=%&\\/~+#-])'\n",
    "    url_repl = ' URL '\n",
    "    text = re.sub(url_pattern, url_repl, text)\n",
    "    # Replace digits\n",
    "    digit_pattern = '\\d+'\n",
    "    digit_repl = ' DIGIT '\n",
    "    text = re.sub(digit_pattern, digit_repl, text)\n",
    "    # Replace hashtags\n",
    "    hashtag_pattern = r'(?:^|\\s)[＃#]{1}(\\w+)'\n",
    "    hashtag_repl = ' HASHTAG '\n",
    "    text = re.sub(hashtag_pattern, hashtag_repl, text)\n",
    "    # Replace users\n",
    "    user_pattern = r'@(\\w+)'\n",
    "    user_repl = ' USER '\n",
    "    text = re.sub(user_pattern, user_repl, text)\n",
    "    # Remove blank spaces\n",
    "    blankspace_pattern = r'\\s+'\n",
    "    blankspace_repl = ' '\n",
    "    text = re.sub(blankspace_pattern, blankspace_repl, text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "728c43fb-13aa-4b1b-a99b-4a92f9404520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lol USER and USER are like soo DIGIT HASHTAG HASHTAG saw it on URL HASHTAG'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test on example tweet\n",
    "preprocess(example_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5f7bb6a-f064-48cc-b650-12c4ef2fbb88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                 USER what USER said.\n",
       "1    USER plus you've added commercials to the expe...\n",
       "2    USER i didn't today... must mean i need to tak...\n",
       "3    USER it's really aggressive to blast obnoxious...\n",
       "4        USER and it's a really big bad thing about it\n",
       "Name: text_processed, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply to text column to create a new column\n",
    "tweets['text_processed'] = tweets['text'].apply(lambda x: preprocess(x))\n",
    "tweets['text_processed'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1576acc6-b305-492a-8fde-65b343cb779c",
   "metadata": {},
   "source": [
    "Preprocessing is complete - time for the bag-of-words!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53282330-54da-4e1c-bfe5-e77cb8fa3add",
   "metadata": {},
   "source": [
    "## The Bag-of-Words Representation\n",
    "\n",
    "The fundamental principle behind bag-of-words is to encode the corpus in terms of word frequencies. Consider the case of sentiment: we know sentiment is conveyed more strongly by specific words. For example, if a tweet contains the word \"happy\", it likely conveys positive sentiment (but not always - someone might say they're \"not happy\" - the opposite sentiment!). Furthermore, when those words come up more often, they'll probably more strongly convey the sentiment.\n",
    "\n",
    "In a bag-of-words, we taken some text, tokenize it, and then tabulate the frequencies of each token. The numerical representation of the text, then, is a vector indicating the frequencies of each token for that text.\n",
    "\n",
    "For example, if we're considering a movie review as follows: \n",
    "\n",
    "![bow](../images/bow.png)\n",
    "\n",
    "We take each token from the review, \"toss it in a bag\", and count up the frequencies of each word. The numerical representation, then, is the vector on the right: the number of appearances of each token. The \"bag\" here denotes that we are not modeling structure within the text - only the frequencies of the words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d9bdbd-406d-469b-a8f6-41d1b3687c37",
   "metadata": {},
   "source": [
    "### Document Term Matrix\n",
    "\n",
    "In most text corpora, we will have many samples or *documents*. For example, in the airline tweets dataset, we have many tweets. Each tweet stands on its own as a unique sample: they can each be thought of as a unique document in the entire *corpus*. Since they are all related to each other, many tokens might be shared across tweets. So, when creating the bag-of-words model, we can tokenize across all documents, forming a *vocabulary*. Then, we can represent a single document by which of the tokens in the vocabulary are represented, and their frequency within the document.\n",
    "\n",
    "If the vocabulary has $V$ tokens, then each document will be encoded in a $V$-dimensional vector. If there are $D$ documents, the entire dataset can be represented in a $D \\times V$ matrix, where each row corresponds to the document, and each column corresponds to the token (or \"term\"). This $D \\times V$ matrix is a **document term matrix** (DTM).\n",
    "\n",
    "Let's consider a simple example. Suppose we have the \"documents\":\n",
    "\n",
    "```\n",
    "[\"You are at a workshop. Are you ready?\",\n",
    " \"Welcome to Berkeley!\",\n",
    " \"I am teaching a workshop.\"]\n",
    "```\n",
    "\n",
    "The unique (word) tokens in this \"corpus\", in alphabetical order, are:\n",
    "\n",
    "```\n",
    "[a, am, are, at, berkeley, i, ready, teaching, to, welcome, workshop, you]\n",
    "```\n",
    "\n",
    "The DTM can be formed by going through each document, ticking off the frequency of each token in each document, and plugging this number into the matrix:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|cccccccccccc}\n",
    " & \\text{a} & \\text{am} & \\text{are} & \\text{at} & \\text{berkeley} & \\text{i} & \\text{ready} & \\text{teaching} & \\text{to} & \\text{welcome} & \\text{workshop} & \\text{you} \\\\\\hline\n",
    "\\text{Document 1} & 1 & 0 & 2 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 \\\\\n",
    "\\text{Document 2} & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 0 & 0 \\\\\n",
    "\\text{Document 3} & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 1 & 0 & 0 & 1 & 0 \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The numerical representation for each document is a row in the matrix. For example, Document 1 has numerical representation $[1, 0, 2, 1, 0, 0, 1, 0, 0, 0, 1, 1]$.\n",
    "\n",
    "To create a DTM, we will use the `CountVectorizer` from the package `sklearn`, a heavily used machine learning package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd2adf56-ba93-459d-8cfa-16ce8dc9284b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f41838-a387-43d0-b4f7-c2e0dba4d050",
   "metadata": {},
   "source": [
    "If you're not familiar with `sklearn`, here is the general workflow:\n",
    "\n",
    "1. We first create a `CountVectorizer` object, and choose specific settings for how we'll go about creating the DTM. Check out the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to see what options are available.\n",
    "2. Then, we \"fit\" this `CountVectorizer` object to the data. In this context, \"fitting\" consists of establishing a vocabulary of tokens from the documents in your dataset.\n",
    "3. Finally, we \"transform\" the data according to the \"fitted\" `CountVectorizer` object. This means taking our text data and transforming it into a DTM according to the vocabulary established by the \"fitting\" step.\n",
    "4. You can do steps 2 and 3 in one fell swoop using a `fit_transform` function.\n",
    "\n",
    "Let's start by creating a `CountVectorizer` with the default settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "783e44a4-4a22-4290-b222-282b02c080dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4f8d2f-c21e-40a8-9ad0-4a8f1a8fe005",
   "metadata": {},
   "source": [
    "Next, we'll fit *and* transform the airline tweets data. What does the documentation say about this function? \n",
    "\n",
    "We need to pass in all the tweets. What is returned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f85e76ea-bc54-4775-bcda-432a03d2c96f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<14640x9913 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 230849 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = vectorizer.fit_transform(tweets['text_processed'])\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8036ca73-2d0b-4781-9f2c-a215c838f4b2",
   "metadata": {},
   "source": [
    "It's not *quite* a `numpy` array. Instead, it's a `numpy` array stored in \"Compressed Sparse Format\". This format saves a lot of memory, but it is difficult to look at for a human. To illustrate the techniques in this lesson we will first uncompress this matrix. For larger datasets, however, you'll want to avoid this, as there are performance benefits to using CSF.\n",
    "\n",
    "Converting to a normal `numpy` array is easy: we use a built-in `todense()` function, and pass this into the `np.array()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87119057-c78c-4eb2-a9d6-3e9f44e4c22b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do not run if you have limited memory - this includes DataHub and Binder\n",
    "np.array(counts.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b33def-90fe-4130-9a26-dbee653b9e1c",
   "metadata": {},
   "source": [
    "There are a lot of zeros here! This makes sense: there are probably a lot of tokens in the vocabulary, and each tweet likely only has a few of the tokens.\n",
    "\n",
    "It would be good to know which column refers to which tokens. Let's create a `pandas` DataFrame which has this information. First, we'll need to get the names of each token - how can we do this? Hint: always read the documentation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99322b85-1a15-46a5-bb80-bb5eaa6eeb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14640, 9913)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_exact_</th>\n",
       "      <th>_wtvd</th>\n",
       "      <th>aa</th>\n",
       "      <th>aaaand</th>\n",
       "      <th>aadavantage</th>\n",
       "      <th>aadv</th>\n",
       "      <th>aadvantage</th>\n",
       "      <th>aal</th>\n",
       "      <th>aaron</th>\n",
       "      <th>ab</th>\n",
       "      <th>...</th>\n",
       "      <th>zero</th>\n",
       "      <th>zig</th>\n",
       "      <th>zip</th>\n",
       "      <th>zippers</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zukes</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 9913 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   _exact_  _wtvd  aa  aaaand  aadavantage  aadv  aadvantage  aal  aaron  ab  \\\n",
       "0        0      0   0       0            0     0           0    0      0   0   \n",
       "1        0      0   0       0            0     0           0    0      0   0   \n",
       "2        0      0   0       0            0     0           0    0      0   0   \n",
       "3        0      0   0       0            0     0           0    0      0   0   \n",
       "4        0      0   0       0            0     0           0    0      0   0   \n",
       "\n",
       "   ...  zero  zig  zip  zippers  zone  zones  zoom  zukes  zurich  zz  \n",
       "0  ...     0    0    0        0     0      0     0      0       0   0  \n",
       "1  ...     0    0    0        0     0      0     0      0       0   0  \n",
       "2  ...     0    0    0        0     0      0     0      0       0   0  \n",
       "3  ...     0    0    0        0     0      0     0      0       0   0  \n",
       "4  ...     0    0    0        0     0      0     0      0       0   0  \n",
       "\n",
       "[5 rows x 9913 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract tokens\n",
    "tokens = vectorizer.get_feature_names_out()\n",
    "# Run this instead if you scikit-learn version is a bit older\n",
    "# tokens = vectorizer.get_feature_names()\n",
    "# Create DTM\n",
    "dtm = pd.DataFrame(data=counts.todense(),\n",
    "                   index=tweets.index,\n",
    "                   columns=tokens)\n",
    "# Look at DTM\n",
    "print(dtm.shape)\n",
    "dtm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeb9e39-93c0-4301-bf9d-c4243a12d96b",
   "metadata": {},
   "source": [
    "What can we do with the DTM? For one, we can see the most frequent tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f432154a-eae0-4723-a797-55f3cfdd71c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user      16500\n",
       "to         8653\n",
       "digit      8427\n",
       "the        6063\n",
       "you        4401\n",
       "for        4001\n",
       "flight     3935\n",
       "on         3815\n",
       "and        3733\n",
       "my         3288\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec2e239-4bb8-4452-a358-cc10c46b8786",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Challenge 3: DTM Data Analysis\n",
    "\n",
    "* Print out the most infrequent words rather than the most frequent words. If you're not sure how, check the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html)!\n",
    "* Print the average number of times each word is used in a tweet.\n",
    "* Which non-hashtag, non-digit token appears the most in any given tweet? How many times does it appear? What is the original tweet?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ebcc05f-22b2-4821-8c6e-ae3d674e948f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_exact_       1\n",
       "minus         1\n",
       "minssss       1\n",
       "minors        1\n",
       "minonhold     1\n",
       "minimal       1\n",
       "mini          1\n",
       "minflight     1\n",
       "mines         1\n",
       "mindset       1\n",
       "miracles      1\n",
       "mimosa        1\n",
       "milestone     1\n",
       "milesaaver    1\n",
       "mild          1\n",
       "milageplus    1\n",
       "milage        1\n",
       "mikes         1\n",
       "mikehertz     1\n",
       "mighty        1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# Print out the most infrequent words rather than the most frequent words.\n",
    "dtm.sum().sort_values(ascending=True).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d348b75a-83bf-4291-bae6-b9e9833401d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user       1.127049\n",
       "to         0.591052\n",
       "digit      0.575615\n",
       "the        0.414139\n",
       "you        0.300615\n",
       "for        0.273292\n",
       "flight     0.268784\n",
       "on         0.260587\n",
       "and        0.254986\n",
       "my         0.224590\n",
       "hashtag    0.221516\n",
       "is         0.193374\n",
       "in         0.176776\n",
       "it         0.164754\n",
       "of         0.145014\n",
       "me         0.131489\n",
       "your       0.119057\n",
       "that       0.118579\n",
       "can        0.113183\n",
       "have       0.112432\n",
       "dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the average number of times each word is used in a tweet.\n",
    "dtm.mean().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e6c0fbe2-79dd-4224-9057-2783feaa4f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3915</th>\n",
       "      <td>lt</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1214</th>\n",
       "      <td>worst</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10173</th>\n",
       "      <td>to</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3412</th>\n",
       "      <td>to</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>to</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6469</th>\n",
       "      <td>to</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13715</th>\n",
       "      <td>to</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10614</th>\n",
       "      <td>the</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14030</th>\n",
       "      <td>to</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10599</th>\n",
       "      <td>to</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       token  number\n",
       "3915      lt       6\n",
       "1214   worst       6\n",
       "10173     to       5\n",
       "3412      to       5\n",
       "792       to       5\n",
       "6469      to       5\n",
       "13715     to       5\n",
       "10614    the       5\n",
       "14030     to       5\n",
       "10599     to       5"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which non-hashtag, non-digit token appears the most in any given tweet? How many times does it appear? What is the original tweet?\n",
    "counts = pd.DataFrame()\n",
    "counts['token'] = dtm.idxmax(axis=1)\n",
    "counts['number'] = dtm.max(axis=1)\n",
    "counts[(counts['token'] != 'digit')\n",
    "       & (counts['token'] != 'hashtag')\n",
    "       & (counts['token'] != 'user')].sort_values(\n",
    "    'number',\n",
    "    ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab0f3184-6415-4052-8599-d5989a3afa2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@united &lt;&lt;&lt;&lt;&lt;&lt;-------- do not reply to emails?'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the original tweet?\n",
    "# Examine index 3915: \"It\"\n",
    "tweets.iloc[3915]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0dcee38a-9290-4ff2-9152-8f63016b0c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@united is the worst. Worst reservation policies. Worst costumer service. Worst worst worst. Congrats, @Delta you're not that bad!\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the original tweet?\n",
    "# Examine index 1214: \"worst\"\n",
    "tweets.iloc[1214]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dba8e37-4880-4565-b6fc-7e7c96958f0f",
   "metadata": {},
   "source": [
    "### Customizing the `CountVectorizer`\n",
    "\n",
    "Recall the documentation on the `CountVectorizer` object: there are many options here on how you can customize the procedure of generating the vocabulary. Let's highlight some of these values:\n",
    "\n",
    "* `lowercase`: If `True`, it will lowercase all text. This was one of our preprocessing steps from before!\n",
    "* `stop_words`: You can choose or provide a list of stop-words. The `\"english\"` option is a built-in stop-word list, but you could provide another (e.g., the stop-word list provided by `nltk`).\n",
    "* `min_df` or `max_df`: The **document frequency** is the fraction of documents that a token appears in. If this value is high (close to 1), a token appears in most documents. If it's low (close to 0), it appears in very few documents. You might want to remove very rare tokens (they could be typos or gibberish) and you might want to remove very common tokens (these could be uninformative words that aren't stop words). These arguments allow you to specify a range of desired document frequencies. They can either be counts or fractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37a0a93e-9dd8-43dc-a82c-06a24bf02bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14640, 5152)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aadv</th>\n",
       "      <th>aadvantage</th>\n",
       "      <th>aal</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abc</th>\n",
       "      <th>abi</th>\n",
       "      <th>abilities</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>...</th>\n",
       "      <th>yup</th>\n",
       "      <th>yvonne</th>\n",
       "      <th>yvr</th>\n",
       "      <th>yyj</th>\n",
       "      <th>yyz</th>\n",
       "      <th>zero</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5152 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aa  aadv  aadvantage  aal  abandoned  abc  abi  abilities  ability  able  \\\n",
       "0   0     0           0    0          0    0    0          0        0     0   \n",
       "1   0     0           0    0          0    0    0          0        0     0   \n",
       "2   0     0           0    0          0    0    0          0        0     0   \n",
       "3   0     0           0    0          0    0    0          0        0     0   \n",
       "4   0     0           0    0          0    0    0          0        0     0   \n",
       "\n",
       "   ...  yup  yvonne  yvr  yyj  yyz  zero  zone  zoom  zurich  zz  \n",
       "0  ...    0       0    0    0    0     0     0     0       0   0  \n",
       "1  ...    0       0    0    0    0     0     0     0       0   0  \n",
       "2  ...    0       0    0    0    0     0     0     0       0   0  \n",
       "3  ...    0       0    0    0    0     0     0     0       0   0  \n",
       "4  ...    0       0    0    0    0     0     0     0       0   0  \n",
       "\n",
       "[5 rows x 5152 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the vectorizer\n",
    "vectorizer = CountVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words='english',\n",
    "    min_df=2,\n",
    "    max_df=0.95)\n",
    "# Fit, transform, and get tokens\n",
    "counts = vectorizer.fit_transform(tweets['text_processed'])\n",
    "tokens = vectorizer.get_feature_names_out()\n",
    "# Run this instead if you scikit-learn version is a bit older\n",
    "# tokens = vectorizer.get_feature_names()\n",
    "# Create dataframe\n",
    "dtm = pd.DataFrame(data=counts.todense(),\n",
    "                   index=tweets.index,\n",
    "                   columns=tokens)\n",
    "print(dtm.shape)\n",
    "dtm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1efefa-058b-4016-ba88-7e4e3aa65538",
   "metadata": {},
   "source": [
    "How did the number of tokens change with our new parameter settings?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8b5145-d505-4e36-9a39-a40d25d8ec6f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Challenge 4: Customizing the Vectorizer with `nltk` inputs\n",
    "\n",
    "If you look at the `CountVectorizer` documentation, you'll see that it can actually accept a custom `tokenizer` and `stop_words` list. \n",
    "\n",
    "Using what you learned in the previous workshop, create a `CountVectorizer` that utilizes the `nltk` word tokenizer and stop word list. How does the resulting DTM look different?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da610560-62c3-48ab-a1b2-25e0b589bc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cf4ff597-caa9-4d0b-92b7-57b59156b8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/blueraspberry/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/blueraspberry/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14640, 5518)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>!</th>\n",
       "      <th>#</th>\n",
       "      <th>$</th>\n",
       "      <th>%</th>\n",
       "      <th>&amp;</th>\n",
       "      <th>'</th>\n",
       "      <th>''</th>\n",
       "      <th>'bluemanity</th>\n",
       "      <th>'blumanity</th>\n",
       "      <th>'customer</th>\n",
       "      <th>...</th>\n",
       "      <th>😢</th>\n",
       "      <th>😤</th>\n",
       "      <th>😥</th>\n",
       "      <th>😩</th>\n",
       "      <th>😩😩😩</th>\n",
       "      <th>😭</th>\n",
       "      <th>😭😭</th>\n",
       "      <th>😳</th>\n",
       "      <th>🙏</th>\n",
       "      <th>🙏🙏🙏✌️✌️✌️🙏🙏🙏</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5518 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   !  #  $  %  &  '  ''  'bluemanity  'blumanity  'customer  ...  😢  😤  😥  😩  \\\n",
       "0  0  0  0  0  0  0   0            0           0          0  ...  0  0  0  0   \n",
       "1  0  0  0  0  0  0   0            0           0          0  ...  0  0  0  0   \n",
       "2  1  0  0  0  0  0   0            0           0          0  ...  0  0  0  0   \n",
       "3  0  0  0  0  1  1   1            0           0          0  ...  0  0  0  0   \n",
       "4  0  0  0  0  0  0   0            0           0          0  ...  0  0  0  0   \n",
       "\n",
       "   😩😩😩  😭  😭😭  😳  🙏  🙏🙏🙏✌️✌️✌️🙏🙏🙏  \n",
       "0    0  0   0  0  0             0  \n",
       "1    0  0   0  0  0             0  \n",
       "2    0  0   0  0  0             0  \n",
       "3    0  0   0  0  0             0  \n",
       "4    0  0   0  0  0             0  \n",
       "\n",
       "[5 rows x 5518 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# Find stop words\n",
    "stop_words = stopwords.words('english')\n",
    "# Create the vectorizer\n",
    "vectorizer = CountVectorizer(\n",
    "    lowercase=True,\n",
    "    tokenizer=word_tokenize,\n",
    "    stop_words=stop_words,\n",
    "    min_df=2,\n",
    "    max_df=0.95)\n",
    "# Fit, transform, and get tokens\n",
    "counts = vectorizer.fit_transform(tweets['text_processed'])\n",
    "tokens = vectorizer.get_feature_names_out()\n",
    "# Make DTM\n",
    "dtm = pd.DataFrame(data=counts.todense(),\n",
    "                   index=tweets.index,\n",
    "                   columns=tokens)\n",
    "print(dtm.shape)\n",
    "dtm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38363398-fdf5-456b-ae3d-cae9d5294140",
   "metadata": {},
   "source": [
    "### Term Frequency-Inverse Document Frequency Scores (TF-IDF)\n",
    "\n",
    "So far, we're relying on word frequencies to give us information about a document. This assumes if a word appears more often in a document, it's more informative. However, this may not always be the case. For example, we already remove stop-words because they are not informative, despite the fact that they appear many times in a document. In the airline tweets, the word `flight` appears many times across the corpus, but is also not that informative, because it appears in many documents. Since we're looking at airline tweets, we shouldn't be surprised to see the word `flight`!\n",
    "\n",
    "To remedy this, we use a weighting scheme called **tf-idf (term frequency-inverse document frequency)**. The big idea behind tf-idf is to weight words not just by their frequency *within* a document, but by their frequency in one document *relative* to the remaining documents. Words that are frequent, but also used in every single document, will not be that informative. We want to identify words that are unevenly distributed across the corpus: these are the words most likely to be informative in a particular document.\n",
    "\n",
    "So, when we construct the DTM, we will be assigning each value a **tf-idf score**. Specifically, term $t$ in document $d$ is assigned tf-idf score as follows:\n",
    "\n",
    "$$\n",
    "\\text{tf-idf}(d, t) = \\text{tf}(d, t) \\times \\text{idf}(t)\n",
    "$$\n",
    "\n",
    "where $\\text{tf}(d, t)$ is the term-frequency of term $t$ in document $d$ and $\\text{idf}(t)$ is the inverse-document frequency of term $t$. \n",
    "\n",
    "When we used the `CountVectorizer` above, we only considered **term frequency**: if a term appeared more times in a document, it was given a higher value in the DTM. Now, we are scaling the term frequency by the **inverse document frequency**: if a term appears in many documents, we give it a lower weight, since it is less \"surprising\". \n",
    "\n",
    "We're not done yet: we still need to define the actual functions. In the `CountVectorizer`, the `tf(d, t)` function was simply $n_{d,t}$, or the number of times term $t$ appeared in document $d$. However, we can use other functions. In this case, we'll use:\n",
    "\n",
    "$$\n",
    "\\text{tf}(d, t) = \\frac{n_{d,t}}{N_d}\n",
    "$$\n",
    "\n",
    "where $N_d$ is the number of terms in document $d$. All we're doing here is normalizing the term frequency used in the `CountVectorizer`. Next, the inverse document frequency can be calculated as\n",
    "\n",
    "$$\n",
    "\\text{idf}(t) = 1 + \\log\\left(\\frac{D}{D_t}\\right)\n",
    "$$\n",
    "\n",
    "where $D$ is the total number of documents, and $D_t$ is the number of documents containing term $t$. If every document contains term $t$, then $\\text{idf}(t) = 1$, and no scaling will happen. If very few documents contain term $t$, then $\\text{idf}(t)$ will be very large, and the term frequency will be scaled up. In practice, the inverse document frequency is computed as \n",
    "\n",
    "$$\n",
    "\\text{idf}(t) = 1 + \\log\\left(\\frac{1 + D}{1 + D_t}\\right)\n",
    "$$\n",
    "\n",
    "to prevent any issues with zero occurrences.\n",
    "\n",
    "We can also create a tf-idf DTM using `sklearn`. We'll use a `TfidfVectorizer` this time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5e32d8a-c42d-475f-aab4-21eca8b1aee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d23916c1-5693-456c-b71d-6d9d78d1e2e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<14640x9913 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 230849 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "dtm = vectorizer.fit_transform(tweets['text_processed'])\n",
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "55e509c8-5402-4be0-9143-0e448fff7066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_exact_</th>\n",
       "      <th>_wtvd</th>\n",
       "      <th>aa</th>\n",
       "      <th>aaaand</th>\n",
       "      <th>aadavantage</th>\n",
       "      <th>aadv</th>\n",
       "      <th>aadvantage</th>\n",
       "      <th>aal</th>\n",
       "      <th>aaron</th>\n",
       "      <th>ab</th>\n",
       "      <th>...</th>\n",
       "      <th>zero</th>\n",
       "      <th>zig</th>\n",
       "      <th>zip</th>\n",
       "      <th>zippers</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zukes</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 9913 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   _exact_  _wtvd   aa  aaaand  aadavantage  aadv  aadvantage  aal  aaron  \\\n",
       "0      0.0    0.0  0.0     0.0          0.0   0.0         0.0  0.0    0.0   \n",
       "1      0.0    0.0  0.0     0.0          0.0   0.0         0.0  0.0    0.0   \n",
       "2      0.0    0.0  0.0     0.0          0.0   0.0         0.0  0.0    0.0   \n",
       "3      0.0    0.0  0.0     0.0          0.0   0.0         0.0  0.0    0.0   \n",
       "4      0.0    0.0  0.0     0.0          0.0   0.0         0.0  0.0    0.0   \n",
       "\n",
       "    ab  ...  zero  zig  zip  zippers  zone  zones  zoom  zukes  zurich   zz  \n",
       "0  0.0  ...   0.0  0.0  0.0      0.0   0.0    0.0   0.0    0.0     0.0  0.0  \n",
       "1  0.0  ...   0.0  0.0  0.0      0.0   0.0    0.0   0.0    0.0     0.0  0.0  \n",
       "2  0.0  ...   0.0  0.0  0.0      0.0   0.0    0.0   0.0    0.0     0.0  0.0  \n",
       "3  0.0  ...   0.0  0.0  0.0      0.0   0.0    0.0   0.0    0.0     0.0  0.0  \n",
       "4  0.0  ...   0.0  0.0  0.0      0.0   0.0    0.0   0.0    0.0     0.0  0.0  \n",
       "\n",
       "[5 rows x 9913 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = pd.DataFrame(dtm.todense(),\n",
    "                     columns=vectorizer.get_feature_names_out(),\n",
    "                     index=tweets.index)\n",
    "# Run this if your scikit-learn version is older\n",
    "#tfidf = pd.DataFrame(dtm.todense(),\n",
    "#                     columns=vectorizer.get_feature_names(),\n",
    "#                     index=tweets.index)\n",
    "tfidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877eb33e-f7dd-4e49-8d2f-eb5d14a36297",
   "metadata": {},
   "source": [
    "Let's look at the 20 words with the highest tf-idf weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3b0ced2b-22b2-4ee5-89b2-2fbba93e10e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "whatsoever    0.245656\n",
       "rly           0.246627\n",
       "aftr          0.248886\n",
       "fnd           0.248886\n",
       "hstg          0.248886\n",
       "med           0.248886\n",
       "bng           0.248886\n",
       "cnceld        0.248886\n",
       "dread         0.250462\n",
       "fantasy       0.255245\n",
       "rapidly       0.255245\n",
       "dpted         0.257093\n",
       "connex        0.261642\n",
       "dkyde         0.263007\n",
       "lick          0.267418\n",
       "shoulda       0.268080\n",
       "miscnx        0.269171\n",
       "runners       0.269171\n",
       "awrd          0.269371\n",
       "savr          0.269371\n",
       "dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.max().sort_values(ascending=True).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da410cb3-a452-441b-a94d-8f751d59d7a6",
   "metadata": {},
   "source": [
    "## Supervised Learning: Sentiment Classification\n",
    "\n",
    "Now that we have a numerical representation of the text, we'd like to *do* something with it. A common task is supervised learning: using the numerical representation to predict some kind of label about the text. Text classification can consist of many types of analysis, such as:\n",
    "\n",
    "* Sentiment analysis\n",
    "* Genre classification\n",
    "* Language identification\n",
    "* Authorship attribution\n",
    "* Spam detection\n",
    "* Document relevancy\n",
    "\n",
    "and many others. How exactly do we go about doing this?\n",
    "\n",
    "Let's consider a toy example:\n",
    "\n",
    "```\"This was the best service! Would love to come again!\"```\n",
    "\n",
    "This is very clearly expressing positive sentiment. But how did we make that judgement?\n",
    "\n",
    "* The review claims the service is the \"best\". This is pretty positive.\n",
    "* The reviewer says they would \"love\" to come again. This is also positive.\n",
    "\n",
    "Specific key words (tokens) were predictive of the sentiment. If someone says something was the \"best\", it's a good sign there's positive sentiment in the text. The semantic meaning of these words helps convey sentiment. \n",
    "\n",
    "This is how we can proceed with classification. We'll construct a DTM from the text data, and use that to predict the labels using a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1121954-e13c-4bdc-9323-6834060d7197",
   "metadata": {},
   "source": [
    "### Predicting Sentiment with Logistic Regression\n",
    "\n",
    "We're going to use a logistic regression model to predict the labels. If you're not familiar with this model, or the basics of machine learning, we recommend taking the Python Machine Learning workshop offered at the D-Lab. For now, we'll review some basics.\n",
    "\n",
    "The DTM is a $D\\times T$ matrix, where $D$ is the number of documents, and $T$ is the number of terms. We can think of the $T$ terms as the \"features\": these values are what we'll use to predict the sentiment. The $D$ documents represent samples: we'll use the patterns across these samples to learn a relationship between the feature and the outcome.\n",
    "\n",
    "In logistic regression, we are learning a **linear model** represented by specific parameters, which we'll call $\\beta_i$. For features $x_i$ (this is a row in the DTM), we construct a **logit**:\n",
    "\n",
    "$$\n",
    "L = \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_T x_T\n",
    "$$\n",
    "\n",
    "and obtain a probability $p$ by passing this logit through the **sigmoid function**, which maps any value onto the range $[0, 1]$:\n",
    "\n",
    "$$\n",
    "p = \\text{sigmoid}(L) = \\frac{1}{1 + \\exp(-L)}\n",
    "$$\n",
    "\n",
    "We can quite literally think of $p$ as a probability that the sample falls in one of the two classes. If, say, $p>0.5$, we call the sample positive sentiment; otherwise, it's negative sentiment.\n",
    "\n",
    "So, to summarize: we take each sample, which consists of features $x = (x_1, x_2, \\ldots, x_T)$, multiply them by $\\beta_i$ values, and add them up. Pass them through a sigmoid, and get a probability. The key question is: how do we know what $\\beta_i$ values to use? We won't discuss thus details here, but an optimization algorithm will learn the best values given the data. \n",
    "\n",
    "Let's train a model! We're going to use `sklearn` to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "33413d63-87eb-489f-b374-3cfeaa51cf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2af16205-d64f-4eed-a882-c914627844e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Did you kernel die from memory issues? \n",
    "# Here's a handy cell to get everything set up from here.\n",
    "# You can restart your kernel and just start from this point. \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from string import punctuation\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Load tweets\n",
    "tweets_path = '../data/airline_tweets.csv'\n",
    "tweets = pd.read_csv(tweets_path, sep=',')\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"Preprocesses a string.\"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Replace URLs\n",
    "    url_pattern = r'(http|ftp|https):\\/\\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\\/~+#-]*[\\w@?^=%&\\/~+#-])'\n",
    "    url_repl = ' URL '\n",
    "    text = re.sub(url_pattern, url_repl, text)\n",
    "    # Replace digits\n",
    "    digit_pattern = '\\d+'\n",
    "    digit_repl = ' DIGIT '\n",
    "    text = re.sub(digit_pattern, digit_repl, text)\n",
    "    # Replace hashtags\n",
    "    hashtag_pattern = r'(?:^|\\s)[＃#]{1}(\\w+)'\n",
    "    hashtag_repl = ' HASHTAG '\n",
    "    text = re.sub(hashtag_pattern, hashtag_repl, text)\n",
    "    # Replace users\n",
    "    user_pattern = r'@(\\w+)'\n",
    "    user_repl = ' USER '\n",
    "    text = re.sub(user_pattern, user_repl, text)\n",
    "    # Remove blank spaces\n",
    "    blankspace_pattern = r'\\s+'\n",
    "    blankspace_repl = ' '\n",
    "    text = re.sub(blankspace_pattern, blankspace_repl, text).strip()\n",
    "    return text\n",
    "\n",
    "tweets['text_processed'] = tweets['text'].apply(lambda x: preprocess(x))\n",
    "example_tweet = \"lol @justinbeiber and @BillGates are like soo 2000 #yesterday #amiright saw it on https://twitter.com #yolo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330d33c6-2640-44cd-8a3a-3dd62b98f3af",
   "metadata": {},
   "source": [
    "To understand the theoretical gist of our classification task, let's first focus on a binary 'positive vs negative' classifier. We are going to do so by restricting the analysis to the non-neutral tweets. So, we'll first partition the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "508769ec-dd07-4492-8308-17e031a522da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11541, 16)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_binary = tweets[tweets['airline_sentiment'] != 'neutral']\n",
    "tweets_binary.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ca64c2-0cfe-4e12-ae44-74c75fae456d",
   "metadata": {},
   "source": [
    "Next, we're going to apply a tf-idf vectorizer to the data. We're going to make use of the `max_features` argument to restrict the number of tokens our model has to consider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9358ed4d-8108-4fef-9a97-0271544bd30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11541, 5000)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "dtm = vectorizer.fit_transform(tweets_binary['text_processed'])\n",
    "X = np.asarray(dtm.todense())\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a6a8e5-da30-4363-8f17-112a0fe94083",
   "metadata": {},
   "source": [
    "Now, we need to get the labels for each of the 11,541 tweets. We can do this by simply extracting the `airline_sentiment` column - `sklearn` is smart enough to handle the details later. Let's also check out the distribution of the labels, to get a sense for a good baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0e7df4f3-0fb0-406f-9240-c9f9761eabea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative    9178\n",
      "positive    2363\n",
      "Name: airline_sentiment, dtype: int64\n",
      "negative    0.795252\n",
      "positive    0.204748\n",
      "Name: airline_sentiment, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "y = tweets_binary['airline_sentiment']\n",
    "print(y.value_counts())\n",
    "print(y.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee87ff74-3fbb-472a-b795-6f4d18fab215",
   "metadata": {},
   "source": [
    "Machine learning fundamentally requires that we learn the $\\beta_i$ values from a **training set**, and then evaluate the performance on a separately held **test set**. This ensures that we actually develop an algorithm that can **generalize**. We'll use the `train_test_split` function from `sklearn` to separate our data into two sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "64cec8b9-14d9-4897-9c02-cc89fcf7b3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066771d8-2f31-4646-9a1b-6d2b1b9b208c",
   "metadata": {},
   "source": [
    "In order to streamline the training process, we've written a `fit_logistic_regression` function you can use to easily train a model given the data inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d46de0b2-af00-4a1d-b4cd-31b96ce545d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_logistic_regression(X, y):\n",
    "    \"\"\"Fits a logistic regression model to provided data.\"\"\"\n",
    "    model = LogisticRegressionCV(\n",
    "        Cs=10,\n",
    "        penalty='l1',\n",
    "        cv=3,\n",
    "        solver='liblinear',\n",
    "        refit=True).fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124aa7ea-1bc1-43e2-beeb-0ba2da9b2df9",
   "metadata": {},
   "source": [
    "We'll fit the model, and see how it performs on both the training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "773963bd-6603-4fad-884b-09ce60afab18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the logistic regression model\n",
    "model = fit_logistic_regression(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e3abaa8c-cbb9-46cf-925a-0248a638f667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9542894280762565\n",
      "Test accuracy: 0.9246427024686011\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training accuracy: {model.score(X_train, y_train)}\")\n",
    "print(f\"Test accuracy: {model.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e186c5-1719-4deb-bdb4-614a9980f058",
   "metadata": {},
   "source": [
    "The model got ~95% accuracy on the training set, and ~92% on the test set - that's pretty good! The similarity between the two performances is also a good sign - it means we were able to generalize pretty well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11b5232-b9f5-471b-8269-ee529a6b1072",
   "metadata": {},
   "source": [
    "### Validating Model Performance on New Tweets\n",
    "\n",
    "Now that we have a trained model, there's nothing stopping us from using it on new data! The `model` object comes equipped with a `predict` function that we can use to evaluate on new text samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "21c7ea47-4d82-4918-9875-d0520358de6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some new tweets\n",
    "new_tweets = [example_tweet,\n",
    "              'omg I am never flying on United again',\n",
    "              'I love @VirginAmerica so much #friendlystaff',\n",
    "              'food on Air France is great!']\n",
    "# First, we need to preprocess them\n",
    "new_tweets_processed = [preprocess(tweet) for tweet in new_tweets]\n",
    "# Next, we need to vectorize them\n",
    "X_new = np.asarray(vectorizer.transform(new_tweets_processed).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bf110a09-e99a-40fa-adef-51b1482da413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive', 'negative', 'positive', 'positive'], dtype=object)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run predictions\n",
    "model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d7f31f1f-1eeb-4811-819a-a6ccba08fe59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.27445537e-01, 5.72554463e-01],\n",
       "       [8.91905961e-01, 1.08094039e-01],\n",
       "       [6.10067183e-04, 9.99389933e-01],\n",
       "       [1.78504108e-01, 8.21495892e-01]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get probabilities\n",
    "model.predict_proba(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ec2ccf-e339-4f08-bc86-9d2aa42864d6",
   "metadata": {},
   "source": [
    "Those predictions look pretty good! Feel free to give the model a try on other tweets you write."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310dac39-4753-4ae8-8dfa-e65e5824cccb",
   "metadata": {},
   "source": [
    "### Interpreting the Model Coefficients\n",
    "\n",
    "The nice thing about logistic regression is that it is **interpretable**. Take a look at the logit again:\n",
    "\n",
    "$$\n",
    "L = \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_T x_T\n",
    "$$\n",
    "\n",
    "If $L$ is very positive, the sigmoid will make sure that the probability $p$ is close to 1 (positive sentiment). If $L$ is very negative, the sigmoid will make sure that the probability $p$ is close to 0 (negative sentiment). So, by looking at the coefficients $\\beta_i$, we can see how each feature (token) impacts the eventual prediction. If $\\beta_i >0$, then the presence of that feature implies positive sentiment, according to the model. If  $\\beta_i < 0$, then the presence of that feature implies negative sentiment, according to the model.\n",
    "\n",
    "So, let's take a look at the fitted coefficients to see if what we see makes sense! We can access them using the `coef_` member, and we can match each coefficient to the tokens from the vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6dcb6ef1-13b3-437e-813c-7118911847a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = model.coef_.ravel()\n",
    "tokens = vectorizer.get_feature_names_out()\n",
    "importance = pd.DataFrame()\n",
    "importance['token'] = tokens\n",
    "importance['coefs'] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e5598e78-924d-41a5-a442-dc9c32ce2df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>coefs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aa</td>\n",
       "      <td>-1.45732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aadv</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aadvantage</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aal</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abandoned</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        token    coefs\n",
       "0          aa -1.45732\n",
       "1        aadv  0.00000\n",
       "2  aadvantage  0.00000\n",
       "3         aal  0.00000\n",
       "4   abandoned  0.00000"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3e63814e-9c0d-4f7a-a5e0-72cca2758d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>coefs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4888</th>\n",
       "      <td>worst</td>\n",
       "      <td>-12.366297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2797</th>\n",
       "      <td>not</td>\n",
       "      <td>-9.585467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3771</th>\n",
       "      <td>rude</td>\n",
       "      <td>-9.388493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2803</th>\n",
       "      <td>nothing</td>\n",
       "      <td>-8.918637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>hours</td>\n",
       "      <td>-8.836287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4884</th>\n",
       "      <td>worries</td>\n",
       "      <td>10.723278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2310</th>\n",
       "      <td>kudos</td>\n",
       "      <td>11.675550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4857</th>\n",
       "      <td>wonderful</td>\n",
       "      <td>11.918034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4378</th>\n",
       "      <td>thanks</td>\n",
       "      <td>14.263034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4375</th>\n",
       "      <td>thank</td>\n",
       "      <td>15.498772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          token      coefs\n",
       "4888      worst -12.366297\n",
       "2797        not  -9.585467\n",
       "3771       rude  -9.388493\n",
       "2803    nothing  -8.918637\n",
       "2014      hours  -8.836287\n",
       "...         ...        ...\n",
       "4884    worries  10.723278\n",
       "2310      kudos  11.675550\n",
       "4857  wonderful  11.918034\n",
       "4378     thanks  14.263034\n",
       "4375      thank  15.498772\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance.sort_values('coefs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eed48ea-fd35-4585-9b98-90456aaee447",
   "metadata": {},
   "source": [
    "This makes sense! Tokens like \"worst\", \"not\", and \"rude\" imply negative sentiment, while tokens like \"thanks\", \"kudos\", \"awesome\" imply positive sentiment. Interestingly, the token \"worries\" implies positive sentiment - what does this tell you about how people typically use this token in their tweets?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7803473d-9544-4e6d-a840-241cd9f810f6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Challenge 5\n",
    "\n",
    "Try developing a **multinomial logistic regression** model, to predict positive, negative, and neutral labels. We've provided you a fitter function below, but it's up to you to create new labels, train-test splits, and perform the fitting and evaluation!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "17933ad2-ddd2-42ab-bebb-d5a344848211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_multinomial_logistic_regression(X, y):\n",
    "    \"\"\"Fits a logistic regression model to provided data.\"\"\"\n",
    "    model = LogisticRegressionCV(\n",
    "        multi_class='multinomial',\n",
    "        Cs=10,\n",
    "        penalty='l1',\n",
    "        cv=3,\n",
    "        solver='saga',\n",
    "        refit=True).fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "697c9e29-0fbd-4bb2-982c-9b195c30f89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14640, 5000)\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "dtm = vectorizer.fit_transform(tweets['text_processed'])\n",
    "X = np.asarray(dtm.todense())\n",
    "y = tweets['airline_sentiment']\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2b4d4510-eee3-4c2f-8d34-25db460ef90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "053d60b6-b078-4ea2-80f3-3486a781d329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/blueraspberry/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/blueraspberry/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/blueraspberry/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mfit_multinomial_logistic_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[54], line 9\u001b[0m, in \u001b[0;36mfit_multinomial_logistic_regression\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_multinomial_logistic_regression\u001b[39m(X, y):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124;03m\"\"\"Fits a logistic regression model to provided data.\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mLogisticRegressionCV\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmultinomial\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ml1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msaga\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m----> 9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrefit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1869\u001b[0m, in \u001b[0;36mLogisticRegressionCV.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1867\u001b[0m     prefer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocesses\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1869\u001b[0m fold_coefs_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1872\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1879\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdual\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1880\u001b[0m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscoring\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintercept_scaling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintercept_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1891\u001b[0m \u001b[43m        \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1892\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1893\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miter_encoded_labels\u001b[49m\n\u001b[1;32m   1894\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfolds\u001b[49m\n\u001b[1;32m   1895\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ml1_ratios_\u001b[49m\n\u001b[1;32m   1896\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1898\u001b[0m \u001b[38;5;66;03m# _log_reg_scoring_path will output different shapes depending on the\u001b[39;00m\n\u001b[1;32m   1899\u001b[0m \u001b[38;5;66;03m# multi_class param, so we need to reshape the outputs accordingly.\u001b[39;00m\n\u001b[1;32m   1900\u001b[0m \u001b[38;5;66;03m# Cs is of shape (n_classes . n_folds . n_l1_ratios, n_Cs) and all the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1907\u001b[0m \u001b[38;5;66;03m#  (n_classes, n_folds, n_Cs . n_l1_ratios) or\u001b[39;00m\n\u001b[1;32m   1908\u001b[0m \u001b[38;5;66;03m#  (1, n_folds, n_Cs . n_l1_ratios)\u001b[39;00m\n\u001b[1;32m   1909\u001b[0m coefs_paths, Cs, scores, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mfold_coefs_)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/joblib/parallel.py:1048\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1040\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[1;32m   1047\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1048\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1049\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/joblib/parallel.py:864\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 864\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/joblib/parallel.py:782\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    781\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 782\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/joblib/_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/joblib/parallel.py:263\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    264\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/joblib/parallel.py:263\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:723\u001b[0m, in \u001b[0;36m_log_reg_scoring_path\u001b[0;34m(X, y, train, test, pos_class, Cs, scoring, fit_intercept, max_iter, tol, class_weight, verbose, solver, penalty, dual, intercept_scaling, multi_class, random_state, max_squared_sum, sample_weight, l1_ratio)\u001b[0m\n\u001b[1;32m    720\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n\u001b[1;32m    721\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m sample_weight[train]\n\u001b[0;32m--> 723\u001b[0m coefs, Cs, n_iter \u001b[38;5;241m=\u001b[39m \u001b[43m_logistic_regression_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mCs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m    \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m    \u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpos_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdual\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintercept_scaling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mintercept_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    745\u001b[0m log_reg \u001b[38;5;241m=\u001b[39m LogisticRegression(solver\u001b[38;5;241m=\u001b[39msolver, multi_class\u001b[38;5;241m=\u001b[39mmulti_class)\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# The score method of Logistic Regression has a classes_ attribute.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:524\u001b[0m, in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[1;32m    521\u001b[0m         alpha \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m C) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m l1_ratio)\n\u001b[1;32m    522\u001b[0m         beta \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m C) \u001b[38;5;241m*\u001b[39m l1_ratio\n\u001b[0;32m--> 524\u001b[0m     w0, n_iter_i, warm_start_sag \u001b[38;5;241m=\u001b[39m \u001b[43msag_solver\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarm_start_sag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_saga\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msaga\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msolver must be one of \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlbfgs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewton-cg\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msag\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m}, got \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m solver\n\u001b[1;32m    545\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:325\u001b[0m, in \u001b[0;36msag_solver\u001b[0;34m(X, y, sample_weight, loss, alpha, beta, max_iter, tol, verbose, random_state, check_input, max_squared_sum, warm_start_mem, is_saga)\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mZeroDivisionError\u001b[39;00m(\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent sag implementation does not handle \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe case step_size * alpha_scaled == 1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[1;32m    324\u001b[0m sag \u001b[38;5;241m=\u001b[39m sag64 \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat64 \u001b[38;5;28;01melse\u001b[39;00m sag32\n\u001b[0;32m--> 325\u001b[0m num_seen, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[43msag\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoef_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintercept_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha_scaled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta_scaled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43msum_gradient_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_memory_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseen_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_seen_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintercept_sum_gradient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintercept_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_saga\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_iter_ \u001b[38;5;241m==\u001b[39m max_iter:\n\u001b[1;32m    350\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    351\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe max_iter was reached which means the coef_ did not converge\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    352\u001b[0m         ConvergenceWarning,\n\u001b[1;32m    353\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = fit_multinomial_logistic_regression(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "10ebf0b3-b9d8-4582-991d-89696a74f52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.5664275956284153\n",
      "Test accuracy: 0.5621584699453552\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training accuracy: {model.score(X_train, y_train)}\")\n",
    "print(f\"Test accuracy: {model.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5efe867-1e98-4e82-8b55-6251061321e5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Challenge 6\n",
    "\n",
    "Create a new fitter function that uses a `RandomForestClassifier`. How is the performance? Check the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) for more details.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ecf07d41-bcf1-4ed2-a699-a53f26a5ff2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_random_forest(X, y):\n",
    "    \"\"\"Fits a random forest model to provided data.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    model = RandomForestClassifier(n_estimators=50).fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a69ddbcb-bb20-4c98-9e9d-897366c3c828",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_binary = tweets[tweets['airline_sentiment'] != 'neutral']\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "dtm = vectorizer.fit_transform(tweets_binary['text_processed'])\n",
    "X = np.asarray(dtm.todense())\n",
    "y = tweets_binary['airline_sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c9ddf997-ab77-494a-8a6d-9039aee2f4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fit_random_forest(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "411ed3b9-553c-4d70-a253-1c7911f862b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9994584055459272\n",
      "Test accuracy: 0.8744045041143352\n"
     ]
    }
   ],
   "source": [
    "# Testing performance\n",
    "print(f\"Training accuracy: {model.score(X_train, y_train)}\")\n",
    "print(f\"Test accuracy: {model.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edffb1b-e0d3-4889-bbbf-4f36fcca25fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
